{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install sentence-transformers\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install --upgrade transformers sentence-transformers\n",
    "!pip install seaborn\n",
    "!pip install tqdm\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv files\n",
    "df1 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\LHS.csv\") #path to file1\n",
    "df2 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\RHS.csv\") #path to file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define columns to compare by converting columns in to string format\n",
    "\n",
    "column1 = 'Description' #column name in df1\n",
    "df1[column1] = df1[column1].astype(str)\n",
    "\n",
    "column2 = 'Description' #column name in df2\n",
    "df2[column2] = df2[column2].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying  tokenization (Generic strategies followed by SBERT - wordpiece, byte pair encoding)\n",
    "tokenizer= model.tokenizer\n",
    "text = \"products do not contain ingredients of meat, fish, fowl, animal by-products, eggs or egg products, milk or milk products, honey or honey bee products. Involve no animal testing of ingredients by supplier, producer, manufacturer or independent party.\"\n",
    "tokens = tokenizer.encode(text, add_special_tokens = True)\n",
    "print(\"Number of Tokens: \", len(tokens))\n",
    "print(\"Tokens\", tokens)\n",
    "\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings for selected rows\n",
    "\n",
    "#function to encode sentences in batches\n",
    "def batch_encode(column, batch_size, model):\n",
    "    embeddings = []\n",
    "    column = column.tolist()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(column), batch_size), desc = \"Encoding Batches\"):\n",
    "            batch = column[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, convert_to_tensor= True,show_progress_bar=True)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "embeddings1 = batch_encode(df1[column1], batch_size=128, model=model)\n",
    "embeddings2 = batch_encode(df2[column2], batch_size=128, model=model)\n",
    "\n",
    "\n",
    "#embeddings1 = model.encode(df1[column1].to_list(), show_progress_bar=True,convert_to_tensor=True)\n",
    "#embeddings2 = model.encode(df2[column2].to_list(), show_progress_bar=True,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1.shape\n",
    "embeddings2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the cosine_similarity_matrix here output will be in range of -1 to 1\n",
    "\n",
    "similarity_matrix = util.cos_sim(embeddings1,embeddings2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the similarity matrix as the out values to range from 0 to 1 \n",
    "normalized_similarity_matrix = (similarity_matrix + 1) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify the Data Types as we have data belonging to different databases \n",
    "\n",
    "\n",
    "type_mapping = {\n",
    "    \"CHAR\": [\"VARCHAR\"],\n",
    "    \"NUMC\": [\"INTEGER\", \"BIGINT\", \"NUMERIC\"],\n",
    "    \"DATS\": [\"DATE\"],\n",
    "    \"TIMS\": [\"TIME\"],\n",
    "    \"DEC\": [\"NUMERIC\", \"FLOAT\"],\n",
    "    \"INT4\": [\"INTEGER\", \"BIGINT\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "mappings_dir = r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\VSC\\DataMapper\\trails\\mappings\"\n",
    "\n",
    "# Load mapping files from the mappings directory\n",
    "def load_mappings(mappings_dir):\n",
    "    mappings = {}\n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            database_name = os.path.splitext(filename)[0]  # Get database name from filename\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "    return mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize data types using mappings\n",
    "def normalize_data_type(data_type, database, mappings):\n",
    "    database_mapping = mappings.get(database, {})\n",
    "    for sql_2023_type, db_types in database_mapping.items():\n",
    "        if data_type.upper() in db_types:\n",
    "            return sql_2023_type\n",
    "    return data_type.upper()  # Return original if no match found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE TO MANY - Below code gets the topN similarities from RHS for particular index on LHS and shows if data types are compatible or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to retrieve top-N most similar sentences with data type validation\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, similarity_matrix, df1, df2, column1, column2, type_mapping):\n",
    "    # Initialize variables\n",
    "    similarities = similarity_matrix[selected_index]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]  # Top-N indices sorted in descending order\n",
    "    \n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']  # LHS field name\n",
    "    lhs_desc = df1.loc[selected_index, column1]        # LHS description\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']    # LHS data type\n",
    "    \n",
    "    # List to store all matches\n",
    "    matches = [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"rhs_field_score\": float(similarities[idx]),\n",
    "            \"rhs_index\": int(df2.index[idx]),\n",
    "            \"rhs_field_name\": df2.loc[idx, 'Attribute'],   # RHS attribute name\n",
    "            \"rhs_field_desc\": df2.loc[idx, column2],       # RHS description\n",
    "            \"rhs_data_type\": df2.loc[idx, 'Data_Type'],    # RHS data type\n",
    "            \"compatibility\": \"Compatible\" if df2.loc[idx, 'Data_Type'] in type_mapping.get(lhs_type, []) else \"Incompatible\"\n",
    "        }\n",
    "        for rank, idx in enumerate(top_indices)  # Loop through top indices\n",
    "    ]\n",
    "\n",
    "    # Construct result dictionary\n",
    "    result = {\n",
    "                f\"lhs_field_index\": selected_index,\n",
    "                f\"lhs_field_name\": lhs_field,\n",
    "                f\"lhs_field_description\": lhs_desc,\n",
    "                f\"lhs_field_data_type\": lhs_type,\n",
    "                \"matches\": matches\n",
    "            }\n",
    "\n",
    "    # Return final JSON output\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "\n",
    "# User inputs\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "# Retrieve and display results\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, normalized_similarity_matrix, df1, df2, column1, column2, type_mapping\n",
    ")\n",
    "\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE TO MANY - Below code gets the topN similarities from RHS for particular index on LHS and shows if data types are compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to retrieve top-N most similar sentences with data type validation\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, similarity_matrix, df1, df2, column1, column2, type_mapping):\n",
    "    # Initialize variables\n",
    "    similarities = similarity_matrix[selected_index]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]  # Top-N indices sorted in descending order\n",
    "    \n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']  # LHS field name\n",
    "    lhs_desc = df1.loc[selected_index, column1]        # LHS description\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']    # LHS data type\n",
    "    \n",
    "    # List to store all matches\n",
    "    matches = [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"rhs_field_score\": float(similarities[idx]),\n",
    "            \"rhs_index\": int(df2.index[idx]),\n",
    "            \"rhs_field_name\": df2.loc[idx, 'Attribute'],   # RHS attribute name\n",
    "            \"rhs_field_desc\": df2.loc[idx, column2],       # RHS description\n",
    "            \"rhs_data_type\": df2.loc[idx, 'Data_Type'],    # RHS data type\n",
    "            \"compatibility\": \"Compatible\" if df2.loc[idx, 'Data_Type'] in type_mapping.get(lhs_type, []) else \"Incompatible\"\n",
    "        }\n",
    "        for rank, idx in enumerate(top_indices)  # Loop through top indices\n",
    "        if (rhs_type := df2.loc[idx, 'Data_Type']) in type_mapping.get(lhs_type, [])\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Construct result dictionary\n",
    "    result = {\n",
    "                f\"lhs_field_index\": selected_index,\n",
    "                f\"lhs_field_name\": lhs_field,\n",
    "                f\"lhs_field_description\": lhs_desc,\n",
    "                f\"lhs_field_data_type\": lhs_type,\n",
    "                \"matches\": matches\n",
    "            }\n",
    "\n",
    "    # Return final JSON output\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "\n",
    "# User inputs\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "# Retrieve and display results\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, normalized_similarity_matrix, df1, df2, column1, column2, type_mapping\n",
    ")\n",
    "\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactored code by utilizing the datatype compatibility filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Unified function to retrieve top-N most similar sentences\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, similarity_matrix, df1, df2, column1, column2, type_mapping, filter_compatible=True):\n",
    "    \n",
    "    # Retrieve similarity scores and top-N indices in descending order\n",
    "    similarities = similarity_matrix[selected_index]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "\n",
    "    # Extract LHS details once\n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = df1.loc[selected_index, column1]\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']\n",
    "\n",
    "    # Generate matches based on the filter_compatible flag\n",
    "    matches = []\n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        rhs_type = df2.loc[idx, 'Data_Type']  # Extract RHS data type\n",
    "        # Check compatibility\n",
    "        if not filter_compatible or rhs_type in type_mapping.get(lhs_type, []):\n",
    "            matches.append({\n",
    "                \"rank\": rank + 1,\n",
    "                \"rhs_field_score\": float(similarities[idx]),\n",
    "                \"rhs_index\": int(df2.index[idx]),\n",
    "                \"rhs_field_name\": df2.loc[idx, 'Attribute'],\n",
    "                \"rhs_field_desc\": df2.loc[idx, column2],\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"compatibility\": \"Compatible\" if rhs_type in type_mapping.get(lhs_type, []) else \"Incompatible\"\n",
    "            })\n",
    "\n",
    "    # Construct and return the result\n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\n",
    "                f\"lhs_field_index\": selected_index,\n",
    "                f\"lhs_field_name\": lhs_field,\n",
    "                f\"lhs_field_description\": lhs_desc,\n",
    "                f\"lhs_field_data_type\": lhs_type,\n",
    "                \"matches\": matches\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# User inputs\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "filter_compatible = input(\"Do you want to filter only compatible matches? (yes/no): \").strip().lower() == \"yes\"\n",
    "\n",
    "# Retrieve results\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, normalized_similarity_matrix, df1, df2, column1, column2, type_mapping, filter_compatible\n",
    ")\n",
    "\n",
    "# Print the output as formatted JSON\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Format result output for selected number of sequential rows from LHS and its selected number of similarities and ther fields based on descending order from RHS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to retrieve top-N most similar sentences with data type validation\n",
    "def retrieve_top_similar_sentences_json(num_sentences, top_n, normalized_similarity_matrix, df1, df2, column1, column2, type_mapping):\n",
    "    results = []  # List to hold all results\n",
    "\n",
    "    for selected_index in range(num_sentences):\n",
    "        # Get similarity scores for the selected sentence\n",
    "        similarities = normalized_similarity_matrix[selected_index]\n",
    "\n",
    "        # Find the indices of the top-N most similar sentences in embeddings2\n",
    "        top_indices = np.argsort(similarities)[::-1]  # Sort indices in descending order\n",
    "\n",
    "        matches = []\n",
    "        lhs_type = df1['Data_Type'].iloc[selected_index]  # Retrieve LHS data type for the selected field\n",
    "\n",
    "        # Process matches in descending order of similarity\n",
    "        for rank, idx in enumerate(top_indices, start=1):\n",
    "            if len(matches) >= top_n:  # Stop if we've already collected top N compatible matches\n",
    "                break\n",
    "\n",
    "            rhs_type = df2['Data_Type'].iloc[idx]  # Retrieve RHS data type\n",
    "\n",
    "            # Check data type compatibility\n",
    "            if rhs_type in type_mapping.get(lhs_type, []):  # Only add compatible matches\n",
    "                matches.append({\n",
    "                    \"rank\": rank,\n",
    "                    \"rhs_field_score\": float(similarities[idx]),\n",
    "                    \"rhs_index\": int(df2.index[idx]),\n",
    "                    \"rhs_field_name\": df2['Attribute'].iloc[idx],  # Assuming 'Attribute' is a column in df2\n",
    "                    \"rhs_field_desc\": df2[column2].iloc[idx],  # Column2 is the RHS description\n",
    "                    \"rhs_data_type\": rhs_type,\n",
    "                    \"compatibility\": \"Compatible\"\n",
    "                })\n",
    "\n",
    "        # Construct result for the LHS field\n",
    "        result = {\n",
    "            f\"lhs_field_{selected_index}\": df1['Field Name'].iloc[selected_index],  # Assuming 'Field Name' is a column in df1\n",
    "            f\"lhs_desc_{selected_index}\": df1[column1].iloc[selected_index],  # Column1 is the LHS description\n",
    "            f\"lhs_data_type_{selected_index}\": lhs_type,\n",
    "            \"matches\": matches\n",
    "        }\n",
    "\n",
    "        # Append to results\n",
    "        results.append(result)\n",
    "\n",
    "    # Return the final output as JSON\n",
    "    final_output = {\"results\": results}\n",
    "    return final_output\n",
    "\n",
    "\n",
    "# User inputs: number of sentences to process and top-N similarities to retrieve\n",
    "num_sentences_to_process = int(input(\"Enter the number of sentences from LHS to process: \") or 3)\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "# Retrieve results in the specified JSON format\n",
    "output_json = retrieve_top_similar_sentences_json(num_sentences_to_process, top_n, normalized_similarity_matrix, df1, df2, column1, column2, type_mapping)\n",
    "\n",
    "# Print the output as formatted JSON\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice from here, dont change above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings_dir = r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\VSC\\DataMapper\\versions\\mappings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mapping files from the mappings directory\n",
    "def load_mappings(mappings_dir):\n",
    "    \"\"\"\n",
    "    Load all JSON mapping files from the specified directory.\n",
    "    Returns a dictionary where the key is the database name and the value is the mapping dictionary.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize data types using mappings\n",
    "#def normalize_data_type(data_type, database, mappings):\n",
    "\"\"\"    Normalize a database-specific data type to a SQL:2023 standard using the provided mappings.\n",
    "If no mapping is found, return the original data type.\n",
    "\"\"\"\n",
    "#   database_mapping = mappings.get(database, {})\n",
    "#    return database_mapping.get(data_type.lower(), data_type.lower())  # Use .lower() for case-insensitive matching\n",
    "\n",
    "def normalize_data_type_with_database(data_type, mappings):\n",
    "    \n",
    "    # Convert the input data type to lowercase for case-insensitive comparison.\n",
    "    data_type_lower = data_type.lower()\n",
    "\n",
    "    # Iterate through all databases and their mappings.\n",
    "    for database, database_mapping in mappings.items():\n",
    "        # Iterate through the database-specific mappings (key-value pairs).\n",
    "        for key, value in database_mapping.items():\n",
    "            # If the data_type matches the key (case-insensitive), return the normalized value and the database name.\n",
    "            if data_type_lower == key.lower():\n",
    "                return value.upper(), database\n",
    "\n",
    "    # If no mapping is found, return the original data type in uppercase and None for the database.\n",
    "    return data_type.upper(), None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load mapping files from the mappings directory\n",
    "def load_mappings(mappings_dir):\n",
    "    \"\"\"\n",
    "    Load all JSON mapping files from the specified directory.\n",
    "    Returns a dictionary where the key is the database name and the value is the mapping dictionary.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "# Function to load compatibilities from a JSON file\n",
    "def load_compatibilities(file_path):\n",
    "    \"\"\"\n",
    "    Load data type compatibilities from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "# Function to normalize data types using mappings\n",
    "\n",
    "def normalize_data_type_with_database(data_type, mappings):\n",
    "    \n",
    "    # Convert the input data type to lowercase for case-insensitive comparison.\n",
    "    data_type_lower = data_type.lower()\n",
    "\n",
    "    # Iterate through all databases and their mappings.\n",
    "    for database, database_mapping in mappings.items():\n",
    "        # Iterate through the database-specific mappings (key-value pairs).\n",
    "        for key, value in database_mapping.items():\n",
    "            # If the data_type matches the key (case-insensitive), return the normalized value and the database name.\n",
    "            if data_type_lower == key.lower():\n",
    "                return value.upper(), database\n",
    "\n",
    "    # If no mapping is found, return the original data type in uppercase and None for the database.\n",
    "    return data_type.upper(), None\n",
    "\n",
    "# Function to check compatibility of data types using compatibilities\n",
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    \"\"\"\n",
    "    Check if two data types are compatible using the provided compatibilities.\n",
    "    \"\"\"\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n",
    "\n",
    "\n",
    "# Function to retrieve top-N most similar fields with data type validation\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, similarity_matrix, df1, df2, column1, column2, type_mapping, filter_compatible=True):\n",
    "    \n",
    "    # Ensure the similarity matrix dimensions match the datasets\n",
    "    if similarity_matrix.shape != (len(df1), len(df2)):\n",
    "        raise ValueError(\"The similarity matrix dimensions must match the LHS and RHS datasets.\")\n",
    "    \n",
    "    similarities = similarity_matrix[selected_index]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]  # Top-N indices sorted in descending order\n",
    "\n",
    "    # Extract metadata for the selected LHS field\n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = df1.loc[selected_index, column1]\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']\n",
    "\n",
    "    # Initialize list to store matches\n",
    "    matches = []\n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        rhs_field = df2.loc[idx, 'Attribute']\n",
    "        rhs_desc = df2.loc[idx, column2]\n",
    "        rhs_type = df2.loc[idx, 'Data_Type']\n",
    "        database_context = df2.loc[idx, 'Database'] if 'Database' in df2.columns else \"default_database\"\n",
    "\n",
    "        # Normalize the RHS data type using the mapping\n",
    "        normalized_rhs_type = normalize_data_type(rhs_type, database_context, type_mapping)\n",
    "\n",
    "        # Normalize both LHS and RHS data types to the same case\n",
    "        lhs_type_normalized = lhs_type.upper()\n",
    "        normalized_rhs_type = normalized_rhs_type.upper()\n",
    "\n",
    "        # Debug prints for validation\n",
    "        #print(f\"LHS Type: {lhs_type}, RHS Type: {rhs_type}, Normalized RHS Type: {normalized_rhs_type}\")\n",
    "        print(f\"LHS Type: {lhs_type} (Normalized: {lhs_type_normalized}), RHS Type: {rhs_type} (Normalized: {normalized_rhs_type})\")\n",
    "\n",
    "      \n",
    "        #compatibility = \"Compatible\" if normalized_rhs_type == lhs_type else \"Incompatible\"\n",
    "        #if filter_compatible and compatibility == \"Incompatible\":\n",
    "        #    print(f\"Filtered out: {rhs_field} (LHS: {lhs_type}, RHS: {normalized_rhs_type})\")\n",
    "\n",
    "        #    continue\n",
    "\n",
    "        # Check compatibility using the compatibilities dictionary\n",
    "        if not are_compatible(lhs_type_normalized, normalized_rhs_type, compatibilities):\n",
    "            print(f\"Filtered out: {rhs_field} (LHS: {lhs_type_normalized}, RHS: {normalized_rhs_type})\")\n",
    "            continue\n",
    "\n",
    "        # Mark as compatible if it passes the check\n",
    "        compatibility = \"Compatible\"\n",
    "\n",
    "        # Append match details, including the database name\n",
    "        matches.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"rhs_field_score\": float(similarities[idx]),\n",
    "            \"rhs_index\": int(df2.index[idx]),\n",
    "            \"rhs_field_name\": rhs_field,\n",
    "            \"rhs_field_desc\": rhs_desc,\n",
    "            \"rhs_data_type\": rhs_type,\n",
    "            \"normalized_rhs_type\": normalized_rhs_type,\n",
    "            \"compatibility\": compatibility,\n",
    "            \"database_name\": database_context if database_context else \"Unknown\" # Include database name\n",
    "        })\n",
    "\n",
    "    # Handle case where no matches are found\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Construct the result dictionary\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "# Load mappings and compatibilities\n",
    "mappings_dir = 'C:/Users/Yasvanth.Pamidi/OneDrive - ENCORA/Desktop/VSC/DataMapper/versions/mappings'\n",
    "type_mapping = load_mappings(mappings_dir)  # Load type mappings dynamically\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)  # Load compatibilities\n",
    "\n",
    "# Example user inputs\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "# Retrieve results and print JSON output\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, similarity_matrix, df1, df2, \"Description\", \"Description\", type_mapping,compatibilities\n",
    ")\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1[\"Data_Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2[\"Data_Type\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixing default database issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load mapping files from the mappings directory\n",
    "def load_mappings(mappings_dir):\n",
    "    \"\"\"\n",
    "    Load all JSON mapping files from the specified directory.\n",
    "    Returns a dictionary where the key is the database name and the value is the mapping dictionary.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "                # Debug: Log each loaded database mapping\n",
    "                print(f\"Debug: Loaded Mapping for Database: {database_name}, Data: {mappings[database_name]}\")\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "\n",
    "# Function to load compatibilities from a JSON file\n",
    "def load_compatibilities(file_path):\n",
    "    \"\"\"\n",
    "    Load data type compatibilities from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "# Function to normalize data types using mappings\n",
    "def normalize_data_type(rhs_type, database, mappings):\n",
    "    \"\"\"\n",
    "    Normalize a database-specific data type to a SQL:2023 standard using the provided mappings.\n",
    "    Return the normalized type and the database name.\n",
    "    \"\"\"\n",
    "    print(f\"Debug: RHS Type: {rhs_type}, Database: {database}\")\n",
    "    print(f\"Debug: Mappings Available: {list(mappings.keys())}\")\n",
    "    \n",
    "    # Iterate through all mappings for all databases\n",
    "    rhs_type_lower = rhs_type.lower()  # Ensure input is in lowercase\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "        # Normalize mapping keys for case-insensitivity\n",
    "        normalized_type = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized_type:\n",
    "            print(f\"Debug: Match Found - Database: {db_name}, Type: {normalized_type}\")\n",
    "            return normalized_type, db_name  # Return normalized type and database name\n",
    "\n",
    "    print(f\"Debug: No Match Found for RHS Type: {rhs_type}\")\n",
    "    return rhs_type.upper(), None  # Return original type in uppercase if no match\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to check compatibility of data types using compatibilities\n",
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    \"\"\"\n",
    "    Check if two data types are compatible using the provided compatibilities.\n",
    "    \"\"\"\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n",
    "\n",
    "\n",
    "# Function to retrieve top-N most similar fields with data type validation\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, similarity_matrix, df1, df2, column1, column2, type_mapping, filter_compatible=True):\n",
    "    \n",
    "    # Ensure the similarity matrix dimensions match the datasets\n",
    "    if similarity_matrix.shape != (len(df1), len(df2)):\n",
    "        raise ValueError(\"The similarity matrix dimensions must match the LHS and RHS datasets.\")\n",
    "    \n",
    "    similarities = similarity_matrix[selected_index]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]  # Top-N indices sorted in descending order\n",
    "\n",
    "    # Extract metadata for the selected LHS field\n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = df1.loc[selected_index, column1]\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']\n",
    "\n",
    "    # Initialize list to store matches\n",
    "    matches = []\n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        rhs_field = df2.loc[idx, 'Attribute']\n",
    "        rhs_desc = df2.loc[idx, column2]\n",
    "        rhs_type = df2.loc[idx, 'Data_Type']\n",
    "        database_context = df2.loc[idx, 'Database'] if 'Database' in df2.columns else \"default_database\"\n",
    "\n",
    "        # Normalize the RHS data type using the mapping\n",
    "        normalized_rhs_type, database_context = normalize_data_type(rhs_type, None, type_mapping)\n",
    "\n",
    "        # Normalize both LHS and RHS data types to the same case\n",
    "        lhs_type_normalized = lhs_type.upper()\n",
    "        normalized_rhs_type = normalized_rhs_type.upper()\n",
    "\n",
    "        # Debug prints for validation\n",
    "        print(f\"LHS Type: {lhs_type} (Normalized: {lhs_type_normalized}), \"\n",
    "            f\"RHS Type: {rhs_type} (Normalized: {normalized_rhs_type}), \"\n",
    "            f\"Database Context: {database_context if database_context else 'Unknown'}\")\n",
    "\n",
    "        # Check compatibility using the compatibilities dictionary\n",
    "        is_compatible = are_compatible(lhs_type_normalized, normalized_rhs_type, compatibilities)\n",
    "\n",
    "        # Debugging compatibility logic\n",
    "        print(f\"Debug: Compatibility Check - LHS: {lhs_type_normalized}, RHS: {normalized_rhs_type}, Compatible: {is_compatible}\")\n",
    "\n",
    "        if not is_compatible:\n",
    "            print(f\"Filtered out: {rhs_field} (LHS: {lhs_type_normalized}, RHS: {normalized_rhs_type})\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # Mark as compatible if it passes the check\n",
    "        compatibility = \"Compatible\"\n",
    "\n",
    "        # Append match details, including the database name\n",
    "        matches.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"rhs_field_score\": float(similarities[idx]),\n",
    "            \"rhs_index\": int(df2.index[idx]),\n",
    "            \"rhs_field_name\": rhs_field,\n",
    "            \"rhs_field_desc\": rhs_desc,\n",
    "            \"rhs_data_type\": rhs_type,\n",
    "            \"normalized_rhs_type\": normalized_rhs_type,\n",
    "            \"compatibility\": compatibility,\n",
    "            \"database_name\": database_context if database_context else \"Unknown\" # Include database name\n",
    "        })\n",
    "\n",
    "    # Handle case where no matches are found\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Construct the result dictionary\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "# Load mappings and compatibilities\n",
    "mappings_dir = 'C:/Users/Yasvanth.Pamidi/OneDrive - ENCORA/Desktop/VSC/DataMapper/versions/mappings'\n",
    "type_mapping = load_mappings(mappings_dir)  # Load type mappings dynamically\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)  # Load compatibilities\n",
    "\n",
    "# Example user inputs\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "# Retrieve results and print JSON output\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, similarity_matrix, df1, df2, \"Description\", \"Description\", type_mapping,compatibilities\n",
    ")\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load mapping files from the mappings directory\n",
    "def load_mappings(mappings_dir):\n",
    "    \"\"\"\n",
    "    Load all JSON mapping files from the specified directory.\n",
    "    Returns a dictionary where the key is the database name and the value is the mapping dictionary.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "                # Debug: Log each loaded database mapping\n",
    "                print(f\"Debug: Loaded Mapping for Database: {database_name}, Data: {mappings[database_name]}\")\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "\n",
    "# Function to load compatibilities from a JSON file\n",
    "def load_compatibilities(file_path):\n",
    "    \"\"\"\n",
    "    Load data type compatibilities from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "# Function to normalize data types using mappings\n",
    "def normalize_data_type(rhs_type, database, mappings):\n",
    "    \"\"\"\n",
    "    Normalize a database-specific data type to a SQL:2023 standard using the provided mappings.\n",
    "    Return the normalized type and the database name.\n",
    "    \"\"\"\n",
    "    print(f\"Debug: RHS Type: {rhs_type}, Database: {database}\")\n",
    "    print(f\"Debug: Mappings Available: {list(mappings.keys())}\")\n",
    "    \n",
    "    # Iterate through all mappings for all databases\n",
    "    rhs_type_lower = rhs_type.lower()  # Ensure input is in lowercase\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "        # Normalize mapping keys for case-insensitivity\n",
    "        normalized_type = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized_type:\n",
    "            print(f\"Debug: Match Found - Database: {db_name}, Type: {normalized_type}\")\n",
    "            return normalized_type, db_name  # Return normalized type and database name\n",
    "\n",
    "    print(f\"Debug: No Match Found for RHS Type: {rhs_type}\")\n",
    "    return rhs_type.upper(), None  # Return original type in uppercase if no match\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to check compatibility of data types using compatibilities\n",
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    \"\"\"\n",
    "    Check if two data types are compatible using the provided compatibilities.\n",
    "    \"\"\"\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n",
    "\n",
    "\n",
    "# Function to retrieve top-N most similar fields with data type validation\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, similarity_matrix, df1, df2, column1, column2, type_mapping, filter_compatible=True):\n",
    "    \n",
    "    # Ensure the similarity matrix dimensions match the datasets\n",
    "    if similarity_matrix.shape != (len(df1), len(df2)):\n",
    "        raise ValueError(\"The similarity matrix dimensions must match the LHS and RHS datasets.\")\n",
    "    \n",
    "    similarities = similarity_matrix[selected_index]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]  # Top-N indices sorted in descending order\n",
    "\n",
    "    # Extract metadata for the selected LHS field\n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = df1.loc[selected_index, column1]\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']\n",
    "\n",
    "    # Initialize list to store matches\n",
    "    matches = []\n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        rhs_field = df2.loc[idx, 'Attribute']\n",
    "        rhs_desc = df2.loc[idx, column2]\n",
    "        rhs_type = df2.loc[idx, 'Data_Type']\n",
    "        database_context = df2.loc[idx, 'Database'] if 'Database' in df2.columns else \"default_database\"\n",
    "\n",
    "        # Normalize the RHS data type using the mapping\n",
    "        normalized_rhs_type, database_context = normalize_data_type(rhs_type, None, type_mapping)\n",
    "\n",
    "        # Normalize both LHS and RHS data types to the same case\n",
    "        lhs_type_normalized = lhs_type.upper()\n",
    "        normalized_rhs_type = normalized_rhs_type.upper()\n",
    "\n",
    "        # Debug prints for validation\n",
    "        print(f\"LHS Type: {lhs_type} (Normalized: {lhs_type_normalized}), \"\n",
    "            f\"RHS Type: {rhs_type} (Normalized: {normalized_rhs_type}), \"\n",
    "            f\"Database Context: {database_context if database_context else 'Unknown'}\")\n",
    "\n",
    "        # Check compatibility using the compatibilities dictionary\n",
    "        is_compatible = are_compatible(lhs_type_normalized, normalized_rhs_type, compatibilities)\n",
    "\n",
    "        # Debugging compatibility logic\n",
    "        print(f\"Debug: Compatibility Check - LHS: {lhs_type_normalized}, RHS: {normalized_rhs_type}, Compatible: {is_compatible}\")\n",
    "\n",
    "        if not is_compatible:\n",
    "            print(f\"Filtered out: {rhs_field} (LHS: {lhs_type_normalized}, RHS: {normalized_rhs_type})\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # Mark as compatible if it passes the check\n",
    "        compatibility = \"Compatible\"\n",
    "\n",
    "        # Append match details, including the database name\n",
    "        matches.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"rhs_field_score\": float(similarities[idx]),\n",
    "            \"rhs_index\": int(df2.index[idx]),\n",
    "            \"rhs_field_name\": rhs_field,\n",
    "            \"rhs_field_desc\": rhs_desc,\n",
    "            \"rhs_data_type\": rhs_type,\n",
    "            \"normalized_rhs_type\": normalized_rhs_type,\n",
    "            \"compatibility\": compatibility,\n",
    "            \"database_name\": database_context if database_context else \"Unknown\" # Include database name\n",
    "        })\n",
    "\n",
    "    # Handle case where no matches are found\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Construct the result dictionary\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "# Load mappings and compatibilities\n",
    "mappings_dir = 'C:/Users/Yasvanth.Pamidi/OneDrive - ENCORA/Desktop/VSC/DataMapper/versions/mappings'\n",
    "type_mapping = load_mappings(mappings_dir)  # Load type mappings dynamically\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)  # Load compatibilities\n",
    "\n",
    "# Example user inputs\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "# Retrieve results and print JSON output\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, similarity_matrix, df1, df2, \"Description\", \"Description\", type_mapping,compatibilities\n",
    ")\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load mapping files from the mappings directory\n",
    "def load_mappings(mappings_dir):\n",
    "    \"\"\"\n",
    "    Load all JSON mapping files from the specified directory.\n",
    "    Returns a dictionary where the key is the database name and the value is the mapping dictionary.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "                # Debug: Log each loaded database mapping\n",
    "                print(f\"Debug: Loaded Mapping for Database: {database_name}, Data: {mappings[database_name]}\")\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "# Function to load compatibilities from a JSON file\n",
    "def load_compatibilities(file_path):\n",
    "    \"\"\"\n",
    "    Load data type compatibilities from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to normalize data types using mappings\n",
    "def normalize_data_type(rhs_type, database, mappings):\n",
    "    \"\"\"\n",
    "    Normalize a database-specific data type to a SQL:2023 standard using the provided mappings.\n",
    "    Return the normalized type and the database name.\n",
    "    \"\"\"\n",
    "    print(f\"Debug: RHS Type: {rhs_type}, Database: {database}\")\n",
    "    print(f\"Debug: Mappings Available: {list(mappings.keys())}\")\n",
    "    \n",
    "    # Iterate through all mappings for all databases\n",
    "    rhs_type_lower = rhs_type.lower()  # Ensure input is in lowercase\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "        # Normalize mapping keys for case-insensitivity\n",
    "        normalized_type = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized_type:\n",
    "            print(f\"Debug: Match Found - Database: {db_name}, Type: {normalized_type}\")\n",
    "            return normalized_type, db_name  # Return normalized type and database name\n",
    "\n",
    "    print(f\"Debug: No Match Found for RHS Type: {rhs_type}\")\n",
    "    return rhs_type.upper(), None  # Return original type in uppercase if no match\n",
    "\n",
    "# Function to check compatibility of data types using compatibilities\n",
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    \"\"\"\n",
    "    Check if two data types are compatible using the provided compatibilities.\n",
    "    \"\"\"\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n",
    "\n",
    "# Function to retrieve top-N most similar fields with data type validation\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, similarity_matrix, df1, df2, column1, column2, type_mapping, compatibilities, filter_compatible=True):\n",
    "    # Ensure the similarity matrix dimensions match the datasets\n",
    "    if similarity_matrix.shape != (len(df1), len(df2)):\n",
    "        raise ValueError(\"The similarity matrix dimensions must match the LHS and RHS datasets.\")\n",
    "    \n",
    "    similarities = similarity_matrix[selected_index]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]  # Top-N indices sorted in descending order\n",
    "\n",
    "    # Extract metadata for the selected LHS field\n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = df1.loc[selected_index, column1]\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']\n",
    "\n",
    "    # Initialize list to store matches\n",
    "    matches = []\n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        rhs_field = df2.loc[idx, 'Attribute']\n",
    "        rhs_desc = df2.loc[idx, column2]\n",
    "        rhs_type = df2.loc[idx, 'Data_Type']\n",
    "        database_context = df2.loc[idx, 'Database'] if 'Database' in df2.columns else \"default_database\"\n",
    "\n",
    "        # Normalize the RHS data type using the mapping\n",
    "        normalized_rhs_type, database_context = normalize_data_type(rhs_type, None, type_mapping)\n",
    "\n",
    "        # Normalize both LHS and RHS data types to the same case\n",
    "        lhs_type_normalized = lhs_type.upper()\n",
    "        normalized_rhs_type = normalized_rhs_type.upper()\n",
    "\n",
    "        # Debug prints for validation\n",
    "        print(f\"LHS Type: {lhs_type} (Normalized: {lhs_type_normalized}), \"\n",
    "            f\"RHS Type: {rhs_type} (Normalized: {normalized_rhs_type}), \"\n",
    "            f\"Database Context: {database_context if database_context else 'Unknown'}\")\n",
    "\n",
    "        # Check compatibility using the compatibilities dictionary\n",
    "        is_compatible = are_compatible(lhs_type_normalized, normalized_rhs_type, compatibilities)\n",
    "\n",
    "        # Debugging compatibility logic\n",
    "        print(f\"Debug: Compatibility Check - LHS: {lhs_type_normalized}, RHS: {normalized_rhs_type}, Compatible: {is_compatible}\")\n",
    "\n",
    "        if not is_compatible:\n",
    "            print(f\"Filtered out: {rhs_field} (LHS: {lhs_type_normalized}, RHS: {normalized_rhs_type})\")\n",
    "            continue\n",
    "\n",
    "        # Mark as compatible if it passes the check\n",
    "        compatibility = \"Compatible\"\n",
    "\n",
    "        # Append match details, including the database name\n",
    "        matches.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"rhs_field_score\": float(similarities[idx]),\n",
    "            \"rhs_index\": int(df2.index[idx]),\n",
    "            \"rhs_field_name\": rhs_field,\n",
    "            \"rhs_field_desc\": rhs_desc,\n",
    "            \"rhs_data_type\": rhs_type,\n",
    "            \"normalized_rhs_type\": normalized_rhs_type,\n",
    "            \"compatibility\": compatibility,\n",
    "            \"database_name\": database_context if database_context else \"Unknown\" # Include database name\n",
    "        })\n",
    "\n",
    "    # Handle case where no matches are found\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Construct the result dictionary\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "# Load mappings and compatibilities\n",
    "mappings_dir = 'C:/Users/Yasvanth.Pamidi/OneDrive - ENCORA/Desktop/VSC/DataMapper/versions/mappings'\n",
    "type_mapping = load_mappings(mappings_dir)  # Load type mappings dynamically\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)  # Load compatibilities\n",
    "\n",
    "# Example user inputs\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "# Retrieve results and print JSON output\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, similarity_matrix, df1, df2, \"Description\", \"Description\", type_mapping, compatibilities\n",
    ")\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python script is designed to analyze field similarities between two datasets while validating data type compatibility using mappings and predefined rules. It begins by importing necessary libraries like `os` for file handling, `json` for working with JSON files, `numpy` for numerical operations, and `pandas` for handling tabular data. The `load_mappings` function dynamically loads database-specific type mappings from a directory containing JSON files, while `load_compatibilities` loads data type compatibility rules from a specific JSON file. The `normalize_data_type` function standardizes database-specific data types into SQL:2023 standards, returning the normalized type and corresponding database context. The compatibility check between data types is handled by `are_compatible`, which verifies if two types align based on the compatibility dictionary. The main processing occurs in `retrieve_top_similar_sentences_json`, which identifies the top-N most similar fields from a similarity matrix between two datasets, validates their data type compatibility, and constructs a detailed JSON output. This function includes safeguards like ensuring similarity matrix dimensions match the datasets, normalizing data types, and filtering out incompatible matches. User inputs, such as the index of the field to process and the number of top matches to retrieve, guide the program's operation. Finally, results are output in JSON format for further analysis. Debugging messages are extensively used to log key steps, such as loading mappings, normalizing data types, and checking compatibility, enhancing traceability and troubleshooting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_mappings(mappings_dir):\n",
    "    mappings = {}\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    return mappings\n",
    "\n",
    "def load_compatibilities(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def normalize_data_type(rhs_type, database, mappings):\n",
    "    rhs_type_lower = rhs_type.lower()\n",
    "    matching_databases = []\n",
    "    normalized_type = None\n",
    "\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "        normalized = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized:\n",
    "            normalized_type = normalized\n",
    "            matching_databases.append(db_name)\n",
    "\n",
    "    if matching_databases:\n",
    "        return normalized_type, matching_databases\n",
    "    return rhs_type.upper(), None\n",
    "\n",
    "\n",
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n",
    "\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, normalized_similarity_matrix, df1, df2, column1, column2, type_mapping, compatibilities, filter_compatible=True):\n",
    "    if normalized_similarity_matrix.shape != (len(df1), len(df2)):\n",
    "        raise ValueError(\"The similarity matrix dimensions must match the LHS and RHS datasets.\")\n",
    "    \n",
    "    similarities = normalized_similarity_matrix[selected_index]\n",
    "    matches = []\n",
    "    for idx in range(len(similarities)):\n",
    "        rhs_field = df2.loc[idx, 'Attribute']\n",
    "        rhs_desc = df2.loc[idx, column2]\n",
    "        rhs_type = df2.loc[idx, 'Data_Type']\n",
    "        database_context = df2.loc[idx, 'Database'] if 'Database' in df2.columns else \"default_database\"\n",
    "        normalized_rhs_type, database_context = normalize_data_type(rhs_type, None, type_mapping)\n",
    "        lhs_type = df1.loc[selected_index, 'Data_Type'].upper()\n",
    "        normalized_rhs_type = normalized_rhs_type.upper()\n",
    "        is_compatible = are_compatible(lhs_type, normalized_rhs_type, compatibilities)\n",
    "        if is_compatible:\n",
    "            matches.append({\n",
    "                \"rank\": None,  # Placeholder to set rank later\n",
    "                \"rhs_index\": int(df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"normalized_rhs_type\": normalized_rhs_type,\n",
    "                \"database_name\": database_context if database_context else \"Unknown\",\n",
    "                \"similarity_score\": float(similarities[idx]),\n",
    "                \"compatibility\": \"Compatible\"\n",
    "            })\n",
    "    matches = sorted(matches, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    matches = matches[:top_n]\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "    lhs_field = df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = df1.loc[selected_index, column1]\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "mappings_dir = 'C:/Users/Yasvanth.Pamidi/OneDrive - ENCORA/Desktop/VSC/DataMapper/versions/mappings'\n",
    "type_mapping = load_mappings(mappings_dir)\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)\n",
    "\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, normalized_similarity_matrix, df1, df2, \"Description\", \"Description\", type_mapping, compatibilities\n",
    ")\n",
    "print(json.dumps(output_json, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
