{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install sentence-transformers\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install --upgrade transformers sentence-transformers\n",
    "!pip install seaborn\n",
    "!pip install tqdm\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv files\n",
    "df1 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\LHS.csv\") #path to file1\n",
    "df2 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\RHS.csv\") #path to file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define columns to compare by converting columns in to string format\n",
    "\n",
    "column1 = 'Description' #column name in df1\n",
    "df1[column1] = df1[column1].astype(str)\n",
    "\n",
    "column2 = 'Description' #column name in df2\n",
    "df2[column2] = df2[column2].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings for selected rows\n",
    "\n",
    "#function to encode sentences in batches\n",
    "def batch_encode(column, batch_size, model):\n",
    "    embeddings = []\n",
    "    column = column.tolist()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(column), batch_size), desc = \"Encoding Batches\"):\n",
    "            batch = column[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, convert_to_tensor= True,show_progress_bar=True)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "embeddings1 = batch_encode(df1[column1], batch_size=128, model=model)\n",
    "embeddings2 = batch_encode(df2[column2], batch_size=128, model=model)\n",
    "\n",
    "\n",
    "#embeddings1 = model.encode(df1[column1].to_list(), show_progress_bar=True,convert_to_tensor=True)\n",
    "#embeddings2 = model.encode(df2[column2].to_list(), show_progress_bar=True,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the cosine_similarity_matrix here output will be in range of -1 to 1\n",
    "\n",
    "similarity_matrix = util.cos_sim(embeddings1,embeddings2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the similarity matrix as the out values to range from 0 to 1 \n",
    "normalized_similarity_matrix = (similarity_matrix + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings_dir = r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\VSC\\DataMapper\\versions\\mappings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_mappings : \n",
    "\n",
    "-> Initializes an empty dictionary to store database mappings\n",
    "\n",
    "-> Checks if the specified directory exists. If not, raises a FileNotFoundError\n",
    "\n",
    "-> Iterates through files in the directory: Selects only .json files, excluding compatibilities.json. Extracts the database name from the filename. Opens the JSON file in read mode, reads its contents using json.load, and stores it in the mappings dictionary under the respective database name.\n",
    "\n",
    "-> Checks if no mappings were loaded and raises a ValueError.\n",
    "\n",
    "-> Returns the mappings dictionary containing database-to-data type mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappings(mappings_dir):\n",
    "    mappings = {} # Initializes an empty dictionary to store database mappings\n",
    "\n",
    "    print(f\"Loading mappings from directory: {mappings_dir}\")\n",
    "\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\") #Checks if the specified directory exists. If not, raises a FileNotFoundError\n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "\n",
    "            print(f\"Processing file: {filename} as database: {database_name}\")\n",
    "            \n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file) #Iterates through files in the directory: Selects only .json files, excluding compatibilities.json. Extracts the database name from the filename. Reads the JSON file and loads its content into the mappings dictionary.\n",
    "\n",
    "                print(f\"Loaded mapping for {database_name}: {mappings[database_name]}\")\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\") #Checks if no mappings were loaded and raises a ValueError.\n",
    "    \n",
    "    print(\"Mappings loaded successfully:\", mappings)\n",
    "    return mappings #Returns the mappings dictionary containing database-to-data type mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_compatibilities: Reads the compatibility JSON file and loads it into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compatibilities(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize_data_type:\n",
    "\n",
    "-> Normalizes the data type of a field by converting it to lowercase\n",
    "\n",
    "-> Prepares a list to store databases where a match is found.\n",
    "\n",
    "-> Iterates through each database in the mappings.\n",
    "\n",
    "-> Converts all keys in the mapping to lowercase to ensure case-insensitive lookup.\n",
    "\n",
    "-> If a match for rhs_type_lower is found, it: Updates normalized_type. Adds the database to matching_databases.\n",
    "\n",
    "-> If matches are found, returns the normalized type and list of matching databases. Otherwise, returns the original type (converted to uppercase) and None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_type(rhs_type, database, mappings):\n",
    "    \n",
    "    print(f\"\\nNormalizing RHS Type: {rhs_type} using mappings\")\n",
    "    \n",
    "    rhs_type_lower = rhs_type.lower()\n",
    "\n",
    "    print(f\"Lowercase RHS Type: {rhs_type_lower}\")\n",
    "\n",
    "    matching_databases = []\n",
    "    normalized_type = None #Normalizes the data type of a field by converting it to lowercase, Prepares a list to store databases where a match is found.\n",
    "\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "\n",
    "        print(f\"Checking database: {db_name}\")\n",
    "\n",
    "        normalized = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized:\n",
    "            normalized_type = normalized\n",
    "            matching_databases.append(db_name)\n",
    "\n",
    "            print(f\"Match found in {db_name}: Normalized Type -> {normalized_type}\")\n",
    "\n",
    "    if matching_databases:\n",
    "        \n",
    "        print(f\"Final Normalized Type: {normalized_type}, Matching Databases: {matching_databases}\")\n",
    "\n",
    "        return normalized_type, matching_databases\n",
    "    \n",
    "    \n",
    "    print(f\"No matches found. Returning RHS Type in uppercase: {rhs_type.upper()}\")\n",
    "\n",
    "    return rhs_type.upper(), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are_compatible: \n",
    "\n",
    "Purpose: Checks if two data types (lhs_type and rhs_type) are compatible.\n",
    "\n",
    "-> Converts both types to uppercase for consistency.\n",
    "\n",
    "-> Checks: 1:If the two types are identical. 2:If rhs_type exists in the compatibility list of lhs_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize_length: \n",
    "\n",
    "Purpose: Converts a length to an integer.\n",
    "\n",
    "Logic: If the length is invalid (e.g., non-numeric or None), it returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_length(length):\n",
    "    try:\n",
    "        return int(length)\n",
    "    except (ValueError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: Classifies a length into a bin based on predefined ranges.\n",
    "\n",
    "Logic:\n",
    "\n",
    "-> Iterates through bins (ranges of lengths).\n",
    "\n",
    "-> Returns the index of the bin where the length falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_length(length, bins):\n",
    "    for i, (lower, upper) in enumerate(bins):\n",
    "        if lower <= length <= upper:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: Divides lengths into num_bins equal ranges (bins).\n",
    "\n",
    "Steps:\n",
    "\n",
    "-> Finds the minimum and maximum lengths.\n",
    "\n",
    "-> Calculates the size of each bin.\n",
    "\n",
    "-> Constructs bins as tuples of (lower bound, upper bound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length_bins(lengths, num_bins=3):\n",
    "    min_length = min(lengths)\n",
    "    max_length = max(lengths)\n",
    "    bin_size = (max_length - min_length) / num_bins\n",
    "    bins = [(min_length + i * bin_size, min_length + (i + 1) * bin_size) for i in range(num_bins)]\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: Determines if two lengths are compatible.\n",
    "\n",
    "Logic:\n",
    "\n",
    "-> If check_length is False, compatibility check is skipped\n",
    "\n",
    "-> Normalizes lengths to ensure valid numerical values.\n",
    "\n",
    "-> Handles missing values by returning False.\n",
    "\n",
    "-> Classifies lengths into bins and checks if they fall into the same bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_length_compatible(lhs_length, rhs_length, bins, check_length=True):\n",
    "    if not check_length:\n",
    "        return True\n",
    "    \n",
    "    lhs_length = normalize_length(lhs_length)\n",
    "    rhs_length = normalize_length(rhs_length)\n",
    "\n",
    "    print(f\"LHS Length: {lhs_length}, RHS Length: {rhs_length}\")\n",
    "    \n",
    "    if lhs_length is None or rhs_length is None:\n",
    "\n",
    "        print(\"One of the lengths is invalid. Returning False.\")\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    lhs_category = classify_length(lhs_length, bins)\n",
    "    rhs_category = classify_length(rhs_length, bins)\n",
    "\n",
    "    print(f\"LHS Category: {lhs_category}, RHS Category: {rhs_category}\")\n",
    "    \n",
    "    return lhs_category == rhs_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert numpy types to native Python types\n",
    "def convert_to_native_type(value):\n",
    "    if isinstance(value, (np.integer, np.int64)):\n",
    "        return int(value)\n",
    "    elif isinstance(value, (np.floating, np.float64)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        return value.tolist()\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_similar_sentences_json(selected_index, top_n, normalized_similarity_matrix, df1, df2, column1, column2, type_mapping, compatibilities, filter_compatible=True):\n",
    "    if normalized_similarity_matrix.shape != (len(df1), len(df2)):\n",
    "\n",
    "        print(f\"Error: Similarity matrix shape {normalized_similarity_matrix.shape} does not match dataframes: LHS {len(df1)}, RHS {len(df2)}\")\n",
    "\n",
    "        raise ValueError(\"The similarity matrix dimensions must match the LHS and RHS datasets.\")\n",
    "    \n",
    "    print(\"Input validation passed. Proceeding...\")\n",
    "    \n",
    "    all_lengths = pd.concat([df1['Length'], df2['Length']]).dropna().tolist()\n",
    "\n",
    "    print(f\"All lengths combined: {all_lengths}\")\n",
    "\n",
    "    bins = create_length_bins(all_lengths, num_bins=3)\n",
    "\n",
    "    print(f\"Length bins created: {bins}\")\n",
    "\n",
    "    similarities = normalized_similarity_matrix[selected_index]\n",
    "    matches = []\n",
    "    for idx in range(len(similarities)):\n",
    "        rhs_field = df2.loc[idx, 'Attribute']\n",
    "        rhs_desc = df2.loc[idx, column2]\n",
    "        rhs_type = df2.loc[idx, 'Data_Type']\n",
    "        rhs_length = df2.loc[idx, 'Length'] # Adding length\n",
    "\n",
    "        print(f\"\\nProcessing RHS Index {idx}:\")\n",
    "        print(f\"  Field: {rhs_field}, Description: {rhs_desc}, Type: {rhs_type}, Length: {rhs_length}\")\n",
    "\n",
    "        database_context = df2.loc[idx, 'Database'] if 'Database' in df2.columns else \"default_database\"\n",
    "        normalized_rhs_type, database_context = normalize_data_type(rhs_type, None, type_mapping)\n",
    "\n",
    "        print(f\"  Normalized RHS Type: {normalized_rhs_type}, Database Context: {database_context}\")\n",
    "\n",
    "        lhs_type = df1.loc[selected_index, 'Data_Type'].upper()\n",
    "        lhs_length = df1.loc[selected_index, 'Length']  # Adding length\n",
    "        normalized_lhs_type, _ = normalize_data_type(lhs_type, None, type_mapping)\n",
    "\n",
    "        print(f\"  LHS Type: {lhs_type}, Normalized LHS Type: {normalized_lhs_type}\")\n",
    "\n",
    "        normalized_rhs_type = normalized_rhs_type.upper()\n",
    "        is_compatible = are_compatible(lhs_type, normalized_rhs_type, compatibilities)\n",
    "        length_compatible = is_length_compatible(lhs_length, rhs_length, bins, check_length=check_length)\n",
    "        if is_compatible and length_compatible:\n",
    "            matches.append({\n",
    "                \"rank\": None,  # Placeholder to set rank later\n",
    "                \"similarity_score\": float(similarities[idx]),\n",
    "                \"rhs_index\": int(df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"normalized_rhs_type\": normalized_rhs_type,\n",
    "                \"database_name\": database_context if database_context else \"Unknown\",\n",
    "                \"compatibility\": \"Compatible\",\n",
    "                \"rhs_length\": convert_to_native_type(rhs_length)  # Adding length \n",
    "            })\n",
    "    matches = sorted(matches, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "\n",
    "    print(\"\\nSorted Matches by Similarity Score:\")\n",
    "\n",
    "    matches = matches[:top_n]\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "    lhs_field = df1.loc[selected_index,'Field Name']\n",
    "    lhs_desc = df1.loc[selected_index, column1]\n",
    "    lhs_type = df1.loc[selected_index, 'Data_Type']\n",
    "    lhs_length = df1.loc[selected_index, 'Length']  # Adding length \n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"normalized_lhs_type\": normalized_lhs_type,\n",
    "        \"lhs_field_length\": convert_to_native_type(lhs_length),  # Adding length\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    return {\"results\": [result]}\n",
    "\n",
    "type_mapping = load_mappings(mappings_dir)\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)\n",
    "\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "\n",
    "check_length = True  #Adding Length\n",
    "\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, normalized_similarity_matrix, df1, df2, \"Description\", \"Description\", type_mapping, compatibilities, filter_compatible=True\n",
    ")\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
