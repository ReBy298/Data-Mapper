{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence-transformers in ./.venv/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.48.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (4.55.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (4.48.3)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.9/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pybloom_live in ./.venv/lib/python3.9/site-packages (4.0.0)\n",
      "Requirement already satisfied: bitarray>=0.3.4 in ./.venv/lib/python3.9/site-packages (from pybloom_live) (3.0.0)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in ./.venv/lib/python3.9/site-packages (from pybloom_live) (3.5.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install sentence-transformers\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install --upgrade transformers sentence-transformers\n",
    "%pip install pybloom_live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: et-xmlfile, openpyxl\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance, ks_2samp\n",
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import jaccard\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from openpyxl import Workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Injection (Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes correctly loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user1 = \"Krithika\"\n",
    "user2 = \"Yasvanth\"\n",
    "user3 = \"Rebeca\"\n",
    "\n",
    "\n",
    "def load_csv_files(user):\n",
    "    if user == \"Krithika\":\n",
    "        df1 = pd.read_csv(r\"C:/Users/Krithika.Patali/DataMapper/LHS_3.csv\")\n",
    "        df2 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/RHS 3.csv\")\n",
    "        mappings_dir = r'C:\\Users\\Krithika.Patali\\DataMapper\\Mappings'\n",
    "        df3 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/synthetic_lhs_data.csv\")\n",
    "        df4 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/synthetic_rhs_data.csv\")\n",
    "    elif user == \"Yasvanth\":\n",
    "        df1 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\metadata files\\LHS.csv\")\n",
    "        df2 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\metadata files\\RHS.csv\")\n",
    "        mappings_dir = r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\VSC\\DataMapper\\versions\\mappings\"\n",
    "        df3 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\synthetic data\\synthetic_lhs_data.csv\")\n",
    "        df4 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\synthetic data\\synthetic_rhs_data.csv\")\n",
    "    elif user == \"Rebeca\":\n",
    "        #df1 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/LHS 2.csv\")\n",
    "        #f2 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/RHS 3.csv\")\n",
    "        #mappings_dir = r'/Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir'\n",
    "        #df3 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/synthetic_lhs_data.csv\")\n",
    "        #df4 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/synthetic_rhs_data.csv\")\n",
    "        df1 = pd.read_excel(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/procore .xlsx\")\n",
    "        df2 = pd.read_excel(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/salesforce_leads.xlsx\")\n",
    "        mappings_dir = r'/Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir'\n",
    "        df3 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/synthetic_procore_data.csv\")\n",
    "        df4 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/synthetic_salesforce_data.csv\")\n",
    "    else:\n",
    "        print(\"Error: User not found\")\n",
    "        return None\n",
    "\n",
    "    return {\"df1\": df1, \"df2\": df2, \"mappings_dir\": mappings_dir, \"df3\": df3, \"df4\": df4}\n",
    "\n",
    "\n",
    "files = load_csv_files(user3)\n",
    "\n",
    "#Acces to the dataframes\n",
    "if files:\n",
    "    df1 = files[\"df1\"]\n",
    "    df2 = files[\"df2\"]\n",
    "    mappings_dir = files[\"mappings_dir\"]\n",
    "    df3 = files[\"df3\"]\n",
    "    df4 = files[\"df4\"]\n",
    "    print(\"Dataframes correctly loaded\")\n",
    "else:\n",
    "    print(\"No dataframes loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappings(mappings_dir):\n",
    "    mappings = {}\n",
    "    print(f\"Loading mappings from directory: {mappings_dir}\")\n",
    "\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            print(f\"Processing file: {filename} as database: {database_name}\")\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "                print(f\"Loaded mapping for {database_name}: {mappings[database_name]}\")\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "    print(\"Mappings loaded successfully:\", mappings)\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compatibilities(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_type(rhs_type, mappings):\n",
    "    if not mappings:\n",
    "        raise ValueError(\"Mappings dictionary is empty or not provided.\")\n",
    "    \n",
    "    rhs_type_lower = rhs_type.lower()\n",
    "    matching_databases = []\n",
    "    normalized_type = None\n",
    "\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "        normalized = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized:\n",
    "            normalized_type = normalized\n",
    "            matching_databases.append(db_name)\n",
    "\n",
    "    if matching_databases:\n",
    "        return normalized_type, matching_databases\n",
    "    \n",
    "    return rhs_type.upper(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_length(length):\n",
    "    try:\n",
    "        return int(length)\n",
    "    except (ValueError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_length_compatible(lhs_length, rhs_length, check_length=True):\n",
    "    if not check_length:\n",
    "        return True\n",
    "    \n",
    "    lhs_length = normalize_length(lhs_length)\n",
    "    rhs_length = normalize_length(rhs_length)\n",
    "    \n",
    "    if lhs_length is None or rhs_length is None:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    return lhs_length <= rhs_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_similarity_score(lhs_length, rhs_length):\n",
    "    lhs_length = normalize_length(lhs_length)\n",
    "    rhs_length = normalize_length(rhs_length)\n",
    "    \n",
    "    if lhs_length is None or rhs_length is None:\n",
    "        return 0  # Return 0 if any length is invalid\n",
    "    \n",
    "    # Calculate the absolute difference between lengths\n",
    "    length_difference = abs(lhs_length - rhs_length)\n",
    "    \n",
    "    # Calculate the similarity score based on the difference\n",
    "    max_length = max(lhs_length, rhs_length)\n",
    "    similarity_score = 1 - (length_difference / max_length)\n",
    "    \n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Compatible data using Length and Data Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mappings from directory: /Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir\n",
      "Processing file: pgsql_sql_2023_map.json as database: pgsql_sql_2023_map\n",
      "Loaded mapping for pgsql_sql_2023_map: {'TEXT': 'VARCHAR', 'INTEGER': 'INT', 'VARCHAR': 'VARCHAR', 'FLOAT': 'FLOAT', 'BOOLEAN': 'BIT', 'BIGINT': 'BIGINT'}\n",
      "Processing file: sap_sql2023_map.json as database: sap_sql2023_map\n",
      "Loaded mapping for sap_sql2023_map: {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT'}\n",
      "Mappings loaded successfully: {'pgsql_sql_2023_map': {'TEXT': 'VARCHAR', 'INTEGER': 'INT', 'VARCHAR': 'VARCHAR', 'FLOAT': 'FLOAT', 'BOOLEAN': 'BIT', 'BIGINT': 'BIGINT'}, 'sap_sql2023_map': {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT'}}\n"
     ]
    }
   ],
   "source": [
    "type_mapping = load_mappings(mappings_dir)\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def filter_compatible_indices(df1, df2, type_mapping, compatibilities):\n",
    "    df1['Normalized_Type'], df1['Matching_Databases'] = zip(*df1['Data_Type'].apply(lambda x: normalize_data_type(x, type_mapping)))\n",
    "    df2['Normalized_Type'], df2['Matching_Databases'] = zip(*df2['Data_Type'].apply(lambda x: normalize_data_type(x, type_mapping)))\n",
    "    \n",
    "    compatible_index = defaultdict(list)\n",
    "    length_score = defaultdict(list)\n",
    "    \n",
    "    for idx1, row1 in df1.iterrows():\n",
    "        lhs_type = row1['Normalized_Type']\n",
    "        lhs_length = row1['Length']\n",
    "\n",
    "        for idx2, row2 in df2.iterrows():\n",
    "            rhs_type = row2['Normalized_Type']\n",
    "            rhs_length = row2['Length']\n",
    "            \n",
    "            if are_compatible(lhs_type, rhs_type, compatibilities) and is_length_compatible(lhs_length, rhs_length, check_length=True):\n",
    "                score = length_similarity_score(lhs_length, rhs_length)\n",
    "                compatible_index[idx1].append(idx2)\n",
    "                length_score[idx1].append(score)\n",
    "                \n",
    "    return compatible_index, length_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(column, batch_size, model):\n",
    "    embeddings = []\n",
    "    column = column.tolist()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(column), batch_size), desc=\"Encoding Batches\"):\n",
    "            batch = column[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(compatible_indices, df1, df2, column1, column2, model, calculated_similarities):\n",
    "    embeddings1 = batch_encode(df1[column1], batch_size=128, model=model)\n",
    "    embeddings2 = batch_encode(df2[column2], batch_size=128, model=model)\n",
    "\n",
    "    similarity_matrix = np.zeros((len(df1), len(df2)))\n",
    "\n",
    "    for idx1, compatible_idxs in compatible_indices.items():\n",
    "        for idx2 in compatible_idxs:\n",
    "            if (idx1, idx2) not in calculated_similarities:\n",
    "                similarity_score = util.cos_sim(embeddings1[idx1], embeddings2[idx2]).item()\n",
    "                similarity_matrix[idx1, idx2] = (similarity_score + 1) / 2  \n",
    "                calculated_similarities.add((idx1, idx2))\n",
    "\n",
    "    return csr_matrix(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native_type(value):\n",
    "    if isinstance(value, (np.integer, np.int64)):\n",
    "        return int(value)\n",
    "    elif isinstance(value, (np.floating, np.float64)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        return value.tolist()\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top similar Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top similar sentences based on similarity and compatibility\n",
    "import heapq\n",
    "\n",
    "\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, sparse_similarity_matrix, filtered_df1, filtered_df2, column1, column2, similarity_threshold, compatible_indices, length_scores):\n",
    "    # Convert the sparse similarity matrix row for the selected index to a dense array and flatten it\n",
    "    similarities = sparse_similarity_matrix[selected_index].toarray().flatten()\n",
    "    # Initialize a list to store matches\n",
    "    matches = []\n",
    "\n",
    "    # Check if the selected index is in the compatible indices dictionary\n",
    "    if selected_index in compatible_indices:  # data type and length compatibility\n",
    "        # Iterate over the compatible indices for the selected index\n",
    "        for idx, length_score in zip(compatible_indices[selected_index], length_scores[selected_index]):   \n",
    "            # Get the similarity score for the current index\n",
    "            similarity_score = similarities[idx]\n",
    "            # Skip if the similarity score is below the threshold\n",
    "            if similarity_score < similarity_threshold:\n",
    "                continue\n",
    "\n",
    "            # Retrieve relevant information from filtered_df2 for the current index\n",
    "            #rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_desc = filtered_df2.loc[idx, column2]\n",
    "            rhs_type = filtered_df2.loc[idx, 'Data_Type']\n",
    "            rhs_length = filtered_df2.loc[idx, 'Length']\n",
    "            matching_databases = filtered_df2.loc[idx, 'Matching_Databases']\n",
    "            \n",
    "            # Append the match information to the matches list\n",
    "            matches.append({\n",
    "                \"rank\": None,\n",
    "                \"similarity_score\": float(similarity_score),\n",
    "                \"rhs_index\": int(filtered_df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"matching_databases\": matching_databases if matching_databases else [\"Unknown\"],\n",
    "                \"rhs_length\": convert_to_native_type(rhs_length),\n",
    "                \"length_score\": float(length_score),\n",
    "                \"compatibility\": \"Compatible\"\n",
    "            })\n",
    "\n",
    "    # Sort the matches by similarity score in descending order and keep only the top N matches\n",
    "    matches = sorted(matches, key=lambda x: (x[\"similarity_score\"], x[\"length_score\"]), reverse=True)[:top_n]\n",
    "        \n",
    "    #matches = sorted(matches, key=lambda x: x[\"similarity_score\"], reverse=True)[:top_n]\n",
    "\n",
    "    # Assign ranks to the matches\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "\n",
    "    # If no matches are found, add a message indicating no compatible matches\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Retrieve relevant information from filtered_df1 for the selected index\n",
    "    lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = filtered_df1.loc[selected_index, column1]\n",
    "    lhs_type = filtered_df1.loc[selected_index, 'Data_Type']\n",
    "    lhs_length = filtered_df1.loc[selected_index, 'Length']\n",
    "\n",
    "    # Create the result dictionary containing the LHS field information and matches\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"lhs_field_length\": convert_to_native_type(lhs_length),\n",
    "        \"matches\": matches\n",
    "    }\n",
    "\n",
    "    # Return the result wrapped in a dictionary with a \"results\" key\n",
    "    return {\"results\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mappings from directory: /Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir\n",
      "Processing file: pgsql_sql_2023_map.json as database: pgsql_sql_2023_map\n",
      "Loaded mapping for pgsql_sql_2023_map: {'TEXT': 'VARCHAR', 'INTEGER': 'INT', 'VARCHAR': 'VARCHAR', 'FLOAT': 'FLOAT', 'BOOLEAN': 'BIT', 'BIGINT': 'BIGINT'}\n",
      "Processing file: sap_sql2023_map.json as database: sap_sql2023_map\n",
      "Loaded mapping for sap_sql2023_map: {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT'}\n",
      "Mappings loaded successfully: {'pgsql_sql_2023_map': {'TEXT': 'VARCHAR', 'INTEGER': 'INT', 'VARCHAR': 'VARCHAR', 'FLOAT': 'FLOAT', 'BOOLEAN': 'BIT', 'BIGINT': 'BIGINT'}, 'sap_sql2023_map': {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Batches: 100%|██████████| 1/1 [00:00<00:00, 44.24it/s]\n",
      "Encoding Batches: 100%|██████████| 1/1 [00:00<00:00, 60.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load mappings and compatibilities\n",
    "# Load the type mappings from the mappings directory\n",
    "type_mapping = load_mappings(mappings_dir)\n",
    "# Define the path to the compatibilities file\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "# Load the compatibilities from the file\n",
    "compatibilities = load_compatibilities(compatibilities_file)\n",
    "\n",
    "# Filter compatible data\n",
    "# Filter the dataframes df1 and df2 to find compatible indices based on type and length\n",
    "compatible_indices, length_score = filter_compatible_indices(df1, df2, type_mapping, compatibilities)\n",
    "\n",
    "# Convert columns to string type\n",
    "# Define the column name for descriptions in df1 and df2\n",
    "column1 = 'Description'\n",
    "# Convert the 'Description' column in df1 to string type\n",
    "df1[column1] = df1[column1].astype(str)\n",
    "column2 = 'Description'\n",
    "# Convert the 'Description' column in df2 to string type\n",
    "df2[column2] = df2[column2].astype(str)\n",
    "\n",
    "# Load the model\n",
    "# Load the SentenceTransformer model for encoding text data\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "\n",
    "\n",
    "# Calculate similarity matrix\n",
    "\n",
    "calculated_similarities = set()\n",
    "\n",
    "# Compute similarity matrix\n",
    "# Compute the cosine similarity matrix between the embeddings of df1 and df2\n",
    "sparse_similarity_matrix = calculate_similarities(compatible_indices, df1, df2, column1, column2, model, calculated_similarities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = sparse_similarity_matrix.max()\n",
    "max_index = sparse_similarity_matrix.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor máximo es 0.6879010796546936 en la posición (5, 11)\n"
     ]
    }
   ],
   "source": [
    "max_row, max_col = np.unravel_index(max_index, sparse_similarity_matrix.shape)\n",
    "print(f\"El valor máximo es {max_value} en la posición ({max_row}, {max_col})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"lhs_field_index\": 5,\n",
      "      \"lhs_field_name\": \"End_Date\",\n",
      "      \"lhs_field_description\": \"The expected end date of the project\",\n",
      "      \"lhs_field_data_type\": \"DATE\",\n",
      "      \"lhs_field_length\": 10,\n",
      "      \"matches\": [\n",
      "        {\n",
      "          \"rank\": 1,\n",
      "          \"similarity_score\": 0.6879010796546936,\n",
      "          \"rhs_index\": 11,\n",
      "          \"rhs_field_name\": \"Last_Activity_Date\",\n",
      "          \"rhs_field_desc\": \"The date of the last activity recorded for the account\",\n",
      "          \"rhs_data_type\": \"Date\",\n",
      "          \"matching_databases\": [\n",
      "            \"Unknown\"\n",
      "          ],\n",
      "          \"rhs_length\": 10,\n",
      "          \"length_score\": 1.0,\n",
      "          \"compatibility\": \"Compatible\"\n",
      "        },\n",
      "        {\n",
      "          \"rank\": 2,\n",
      "          \"similarity_score\": 0.6131220683455467,\n",
      "          \"rhs_index\": 10,\n",
      "          \"rhs_field_name\": \"Created_Date\",\n",
      "          \"rhs_field_desc\": \"The date the account was created in Salesforce\",\n",
      "          \"rhs_data_type\": \"Date\",\n",
      "          \"matching_databases\": [\n",
      "            \"Unknown\"\n",
      "          ],\n",
      "          \"rhs_length\": 10,\n",
      "          \"length_score\": 1.0,\n",
      "          \"compatibility\": \"Compatible\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# User Input with Validations\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "#similarity_threshold = float(input(\"Enter the similarity score threshold (0.0 - 1.0): \"))\n",
    "similarity_threshold = 0.5\n",
    "# Retrieve top similar sentences\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, sparse_similarity_matrix, df1, df2, column1, column2, similarity_threshold, compatible_indices, length_score)\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data_Type\n",
       "String     5\n",
       "DATE       2\n",
       "Decimal    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Data_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data_Type\n",
       "String     10\n",
       "Date        2\n",
       "Decimal     1\n",
       "Integer     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Data_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert series to numeric values\n",
    "def convert_numeric_values(series):\n",
    "    try:\n",
    "        # Attempt to convert the series to numeric values, coercing errors to NaN\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "    except Exception as e:\n",
    "        # Print an error message if conversion fails\n",
    "        print(f\"Conversion error: {e}\")\n",
    "        return series\n",
    "\n",
    "# Function to compute statistical similarity between two columns\n",
    "def compute_statistical_similarity(lhs_col, rhs_col, dtype_category):\n",
    "    \"\"\"\n",
    "    Determines if two data distributions are statistically similar based on their type.\n",
    "    \"\"\"\n",
    "    # Initialize similarity score to 0\n",
    "    similarity_score = 0\n",
    "    \n",
    "    # Drop NaN values from both columns\n",
    "    lhs_values = lhs_col.dropna()\n",
    "    rhs_values = rhs_col.dropna()\n",
    "    \n",
    "    # Check if the data type category is numeric\n",
    "    if dtype_category in [\"INT\", \"INTEGER\", \"BIGINT\", \"SMALLINT\", \"FLOAT\", \"DOUBLE\", \"REAL\", \"NUMERIC\", \"DECIMAL\"]:\n",
    "        # Convert columns to numeric values\n",
    "        lhs_values = convert_numeric_values(lhs_col.dropna())\n",
    "        rhs_values = convert_numeric_values(rhs_col.dropna())\n",
    "        \n",
    "        # Compute Wasserstein distance and Kolmogorov-Smirnov statistic if both columns have values\n",
    "        if len(lhs_values) > 0 and len(rhs_values) > 0:\n",
    "            wasserstein_dist = stats.wasserstein_distance(lhs_values, rhs_values)\n",
    "            ks_stat, ks_p_value = stats.ks_2samp(lhs_values, rhs_values)\n",
    "            # Calculate similarity score based on Wasserstein distance and KS statistic\n",
    "            similarity_score = 1 / (1 + wasserstein_dist) if wasserstein_dist > 0 else 1\n",
    "            similarity_score = min(similarity_score, 1 - ks_stat)  # Normalize\n",
    "        else:\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    \n",
    "    # Check if the data type category is categorical\n",
    "    elif dtype_category in [\"CHAR\", \"VARCHAR\", \"TEXT\", \"STRING\", \"JSON\"]:\n",
    "        # Convert columns to unique string values\n",
    "        lhs_values = lhs_col.dropna().astype(str).unique()\n",
    "        rhs_values = rhs_col.dropna().astype(str).unique()\n",
    "        \n",
    "        # Compute Jaccard similarity if both columns have values\n",
    "        if len(lhs_values) > 0 and len(rhs_values) > 0:\n",
    "            vectorizer = CountVectorizer(preprocessor=lambda x: x, binary=True)\n",
    "            matrix = vectorizer.fit_transform([' '.join(lhs_values), ' '.join(rhs_values)])\n",
    "            similarity_score = 1 - jaccard(matrix.toarray()[0], matrix.toarray()[1])\n",
    "        else:\n",
    "            print(\"something happen\")\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    elif dtype_category in [\"DATE\", \"TIMESTAMP\", \"TIME\", \"DATETIME\"]:\n",
    "\n",
    "        # Convert columns to datetime values\n",
    "        lhs_values = pd.to_datetime(lhs_col, errors='coerce')\n",
    "        rhs_values = pd.to_datetime(rhs_col, errors='coerce')\n",
    "        \n",
    "        # Drop NaN values from both columns\n",
    "        lhs_values = lhs_values.dropna()\n",
    "        rhs_values = rhs_values.dropna()\n",
    "        \n",
    "        # Compute similarity score based on min and max date values\n",
    "        if not lhs_values.empty and not rhs_values.empty:\n",
    "            min_date_col1 = lhs_values.min()\n",
    "            max_date_col1 = lhs_values.max()\n",
    "            min_date_col2 = rhs_values.min()\n",
    "            max_date_col2 = rhs_values.max()\n",
    "            \n",
    "            # Calculate similarity score based on date range overlap\n",
    "            similarity_score = 1 - (abs((min_date_col1 - min_date_col2).days) + abs((max_date_col1 - max_date_col2).days)) / 365\n",
    "        else:\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    else:\n",
    "        print(\"NOT WORK FOR THIS DATA TYPE\")\n",
    "        similarity_score = 0  # Incompatible types\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "# Function to truncate percentage values to two decimal places\n",
    "def truncate_percentage(value):\n",
    "    return f\"{value:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top similar sentences based on similarity and statistical compatibility\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, sparse_similarity_matrix, filtered_df1, filtered_df2, column1, column2, similarity_threshold, statistical_threshold, compatible_indices, length_scores):\n",
    "    # Convert the sparse similarity matrix row for the selected index to a dense array and flatten it\n",
    "    similarities = sparse_similarity_matrix[selected_index].toarray().flatten()\n",
    "    # Initialize a list to store matches\n",
    "    matches = []\n",
    "\n",
    "    # Check if the selected index is in the compatible indices dictionary\n",
    "    if selected_index in compatible_indices:\n",
    "        # Iterate over the compatible indices for the selected index\n",
    "        for idx, length_score in zip(compatible_indices[selected_index], length_scores[selected_index]):  \n",
    "            # Get the similarity score for the current index\n",
    "            similarity_score = similarities[idx]\n",
    "            # Skip if the similarity score is below the threshold\n",
    "            if similarity_score < similarity_threshold:\n",
    "                continue\n",
    "\n",
    "            # Retrieve relevant information from filtered_df2 for the current index\n",
    "            rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_desc = filtered_df2.loc[idx, column2]\n",
    "            rhs_type = filtered_df2.loc[idx, 'Data_Type']\n",
    "            normalized_rhs_type = filtered_df2.loc[idx, 'Normalized_Type']\n",
    "            rhs_length = filtered_df2.loc[idx, 'Length']\n",
    "            matching_databases = filtered_df2.loc[idx, 'Matching_Databases']\n",
    "            \n",
    "            # Retrieve the field name from filtered_df1 for the selected index\n",
    "            lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "\n",
    "            # Check if the lhs_field and rhs_field exist in df3 and df4 respectively\n",
    "            if lhs_field in df3.columns and rhs_field in df4.columns:\n",
    "                # Retrieve the columns from df3 and df4\n",
    "                lhs_col = df3[lhs_field]\n",
    "                rhs_col = df4[rhs_field]\n",
    "                # Compute the statistical similarity between the columns\n",
    "                stat_similarity = compute_staxtistical_similarity(lhs_col, rhs_col, normalized_rhs_type)\n",
    "            else:\n",
    "                stat_similarity = 0  # No valid comparison\n",
    "                \n",
    "            # Convert the statistical similarity to a percentage\n",
    "            stat_similarity_percentage = truncate_percentage(stat_similarity * 100)\n",
    "            \n",
    "            # Skip if the statistical similarity is below the threshold\n",
    "            if stat_similarity < statistical_threshold:\n",
    "                continue\n",
    "            \n",
    "                \n",
    "            # Append the match information to the matches list\n",
    "            matches.append({\n",
    "                \"rank\": None,\n",
    "                \"similarity_score\": float(similarity_score),\n",
    "                \"rhs_index\": int(filtered_df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"matching_databases\": matching_databases if matching_databases else [\"Unknown\"],\n",
    "                \"rhs_length\": convert_to_native_type(rhs_length),\n",
    "                \"length_score\": float(length_score),\n",
    "                \"statistical_similarity\": stat_similarity_percentage  \n",
    "            })\n",
    "\n",
    "    # Sort the matches by similarity score in descending order and keep only the top N matches\n",
    "    #matches = sorted(matches, key=lambda x: x[\"similarity_score\"], reverse=True)[:top_n]\n",
    "    matches = sorted(matches, key=lambda x: (x[\"similarity_score\"],x[\"length_score\"], x[\"statistical_similarity\"]), reverse=True)[:top_n]\n",
    "\n",
    "    # Assign ranks to the matches\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "\n",
    "    # If no matches are found, add a message indicating no compatible matches\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Retrieve relevant information from filtered_df1 for the selected index\n",
    "    lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = filtered_df1.loc[selected_index, column1]\n",
    "    lhs_type = filtered_df1.loc[selected_index, 'Data_Type']\n",
    "    lhs_length = filtered_df1.loc[selected_index, 'Length']\n",
    "\n",
    "    # Create the result dictionary containing the LHS field information and matches\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"lhs_field_length\": convert_to_native_type(lhs_length),\n",
    "        \"matches\": matches\n",
    "    }\n",
    "\n",
    "    # Return the result wrapped in a dictionary with a \"results\" key\n",
    "    return {\"results\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"lhs_field_index\": 5,\n",
      "      \"lhs_field_name\": \"End_Date\",\n",
      "      \"lhs_field_description\": \"The expected end date of the project\",\n",
      "      \"lhs_field_data_type\": \"DATE\",\n",
      "      \"lhs_field_length\": 10,\n",
      "      \"matches\": [\n",
      "        {\n",
      "          \"rank\": 1,\n",
      "          \"similarity_score\": 0.6879010796546936,\n",
      "          \"rhs_index\": 11,\n",
      "          \"rhs_field_name\": \"Last_Activity_Date\",\n",
      "          \"rhs_field_desc\": \"The date of the last activity recorded for the account\",\n",
      "          \"rhs_data_type\": \"Date\",\n",
      "          \"matching_databases\": [\n",
      "            \"Unknown\"\n",
      "          ],\n",
      "          \"rhs_length\": 10,\n",
      "          \"length_score\": 1.0,\n",
      "          \"statistical_similarity\": \"88.49\"\n",
      "        },\n",
      "        {\n",
      "          \"rank\": 2,\n",
      "          \"similarity_score\": 0.6131220683455467,\n",
      "          \"rhs_index\": 10,\n",
      "          \"rhs_field_name\": \"Created_Date\",\n",
      "          \"rhs_field_desc\": \"The date the account was created in Salesforce\",\n",
      "          \"rhs_data_type\": \"Date\",\n",
      "          \"matching_databases\": [\n",
      "            \"Unknown\"\n",
      "          ],\n",
      "          \"rhs_length\": 10,\n",
      "          \"length_score\": 1.0,\n",
      "          \"statistical_similarity\": \"81.92\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# User Input with Validations\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "#similarity_threshold = float(input(\"Enter the similarity score threshold (0.0 - 1.0): \"))\n",
    "#statistical_threshold = float(input(\"Enter the statistical similarity score threshold (0.0 - 1.0): \"))\n",
    "\n",
    "similarity_threshold = 0.5\n",
    "statistical_threshold = 0.5\n",
    "\n",
    "# Retrieve top similar sentences\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, sparse_similarity_matrix, df1, df2, column1, column2, similarity_threshold,statistical_threshold, compatible_indices, length_score)\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Null Values</th>\n",
       "      <th>Distinct Values</th>\n",
       "      <th>Non-Distinct Values</th>\n",
       "      <th>Most Common Value</th>\n",
       "      <th>Mean Length</th>\n",
       "      <th>Min Length</th>\n",
       "      <th>Max Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BWVOR</td>\n",
       "      <td>0</td>\n",
       "      <td>727</td>\n",
       "      <td>61</td>\n",
       "      <td>Ahead attack method.</td>\n",
       "      <td>16.922589</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tobacco / Alcohol Age restriction Indicator</td>\n",
       "      <td>0</td>\n",
       "      <td>1355</td>\n",
       "      <td>110</td>\n",
       "      <td>Instead cultural.</td>\n",
       "      <td>16.892150</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple supplier Share Target percentage</td>\n",
       "      <td>0</td>\n",
       "      <td>1341</td>\n",
       "      <td>124</td>\n",
       "      <td>Design effect so.</td>\n",
       "      <td>16.897611</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Distributor retail value by  Pricelist for mul...</td>\n",
       "      <td>0</td>\n",
       "      <td>1330</td>\n",
       "      <td>135</td>\n",
       "      <td>World program whom.</td>\n",
       "      <td>16.937884</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Column  Null Values  \\\n",
       "0                                              BWVOR            0   \n",
       "1        Tobacco / Alcohol Age restriction Indicator            0   \n",
       "2          Multiple supplier Share Target percentage            0   \n",
       "3  Distributor retail value by  Pricelist for mul...            0   \n",
       "\n",
       "   Distinct Values  Non-Distinct Values     Most Common Value  Mean Length  \\\n",
       "0              727                   61  Ahead attack method.    16.922589   \n",
       "1             1355                  110     Instead cultural.    16.892150   \n",
       "2             1341                  124     Design effect so.    16.897611   \n",
       "3             1330                  135   World program whom.    16.937884   \n",
       "\n",
       "   Min Length  Max Length  \n",
       "0           8          20  \n",
       "1           7          20  \n",
       "2           6          20  \n",
       "3           7          20  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Top Patterns</th>\n",
       "      <th>Inferred Data Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BWVOR</td>\n",
       "      <td>object</td>\n",
       "      <td>[Blue, Start, Heart, Wide, Recent, Doctor, Rec...</td>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tobacco / Alcohol Age restriction Indicator</td>\n",
       "      <td>object</td>\n",
       "      <td>[Recent, Raise, Player, West, Teacher, None, T...</td>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple supplier Share Target percentage</td>\n",
       "      <td>object</td>\n",
       "      <td>[Worry, Dog, These, Indicate, Tree, Long, Old,...</td>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Distributor retail value by  Pricelist for mul...</td>\n",
       "      <td>object</td>\n",
       "      <td>[Indicate, Mouth, Serve, City, Onto, Century, ...</td>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Column Data Type  \\\n",
       "0                                              BWVOR    object   \n",
       "1        Tobacco / Alcohol Age restriction Indicator    object   \n",
       "2          Multiple supplier Share Target percentage    object   \n",
       "3  Distributor retail value by  Pricelist for mul...    object   \n",
       "\n",
       "                                        Top Patterns Inferred Data Domain  \n",
       "0  [Blue, Start, Heart, Wide, Recent, Doctor, Rec...          Categorical  \n",
       "1  [Recent, Raise, Player, West, Teacher, None, T...          Categorical  \n",
       "2  [Worry, Dog, These, Indicate, Tree, Long, Old,...          Categorical  \n",
       "3  [Indicate, Mouth, Serve, City, Onto, Century, ...          Categorical  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import IPython.display as display\n",
    "\n",
    "\n",
    "# Load data into DataFrames\n",
    "lhs_df = df3\n",
    "rhs_df = df4\n",
    "\n",
    "# Define columns of interest\n",
    "lhs_column = output_json[\"results\"][0][\"lhs_field_name\"]\n",
    "if \"message\" not in output_json[\"results\"][0][\"matches\"][0]:\n",
    "    rhs_columns = [match[\"rhs_field_name\"] for match in output_json[\"results\"][0][\"matches\"]]\n",
    "else:\n",
    "    rhs_columns = []\n",
    "\n",
    "# Extract relevant columns\n",
    "lhs_data = lhs_df[[lhs_column]]\n",
    "rhs_data = rhs_df[rhs_columns]\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = pd.DataFrame(columns=[\"Column\", \"Null Values\", \"Distinct Values\", \"Non-Distinct Values\", \"Most Common Value\", \"Mean Length\", \"Min Length\", \"Max Length\"])\n",
    "\n",
    "for col in [lhs_column] + rhs_columns:\n",
    "    null_values = lhs_df[col].isnull().sum() if col == lhs_column else rhs_df[col].isnull().sum()\n",
    "    distinct_values = lhs_df[col].nunique() if col == lhs_column else rhs_df[col].nunique()\n",
    "    total_values = len(lhs_df[col]) if col == lhs_column else len(rhs_df[col])\n",
    "    non_distinct_values = total_values - distinct_values\n",
    "    most_common_value = lhs_df[col].mode()[0] if col == lhs_column else rhs_df[col].mode()[0]\n",
    "    \n",
    "    mean_length = lhs_df[col].astype(str).str.len().mean() if col == lhs_column else rhs_df[col].astype(str).str.len().mean()\n",
    "    min_length = lhs_df[col].astype(str).str.len().min() if col == lhs_column else rhs_df[col].astype(str).str.len().min()\n",
    "    max_length = lhs_df[col].astype(str).str.len().max() if col == lhs_column else rhs_df[col].astype(str).str.len().max()\n",
    "    \n",
    "    summary_stats.loc[len(summary_stats)] = [col, null_values, distinct_values, non_distinct_values, most_common_value, mean_length, min_length, max_length]\n",
    "\n",
    "# Display summary statistics\n",
    "display.display(summary_stats)\n",
    "\n",
    "# Generate detailed view with inferred patterns and data types\n",
    "detailed_view = pd.DataFrame(columns=[\"Column\", \"Data Type\", \"Top Patterns\", \"Inferred Data Domain\"])\n",
    "\n",
    "for col in [lhs_column] + rhs_columns:\n",
    "    data_type = lhs_df[col].dtype if col == lhs_column else rhs_df[col].dtype\n",
    "    inferred_patterns = lhs_df[col].astype(str).str.extract(r'([A-Za-z]+)')[0].dropna().unique() if col == lhs_column else rhs_df[col].astype(str).str.extract(r'([A-Za-z]+)')[0].dropna().unique()\n",
    "    inferred_data_domain = \"Categorical\" if data_type == 'object' else \"Numerical\"\n",
    "    \n",
    "    detailed_view.loc[len(detailed_view)] = [col, data_type, inferred_patterns, inferred_data_domain]\n",
    "\n",
    "# Display detailed view\n",
    "display.display(detailed_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
