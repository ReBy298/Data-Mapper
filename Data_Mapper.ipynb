{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ympy (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2024.12.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ympy (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sympy\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install sentence-transformers\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install --upgrade transformers sentence-transformers\n",
    "%pip install pybloom_live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: openpyxl in ./.venv/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (1.13.1)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy) (1.3.0)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sympy\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeca.mendoza/Desktop/Data_Mapper/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance, ks_2samp\n",
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import jaccard\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from openpyxl import Workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Injection (Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes correctly loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user1 = \"Krithika\"\n",
    "user2 = \"Yasvanth\"\n",
    "user3 = \"Rebeca\"\n",
    "\n",
    "\n",
    "def load_csv_files(user):\n",
    "    if user == \"Krithika\":\n",
    "        df1 = pd.read_csv(r\"C:/Users/Krithika.Patali/DataMapper/LHS_3.csv\")\n",
    "        df2 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/RHS 3.csv\")\n",
    "        mappings_dir = r'C:\\Users\\Krithika.Patali\\DataMapper\\Mappings'\n",
    "        df3 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/synthetic_lhs_data.csv\")\n",
    "        df4 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/synthetic_rhs_data.csv\")\n",
    "    elif user == \"Yasvanth\":\n",
    "        df1 = pd.read_excel(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\procore .xlsx\")\n",
    "        df2 = pd.read_excel(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\salesforce_leads.xlsx\")\n",
    "        mappings_dir = r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\VSC\\DataMapper\\versions\\mappings\"\n",
    "        df3 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\synthetic_procore_data.csv\")\n",
    "        df4 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\synthetic_salesforce_data.csv\")\n",
    "    elif user == \"Rebeca\":\n",
    "        df1 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/LHS 2.csv\")\n",
    "        f2 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/RHS 3.csv\")\n",
    "        mappings_dir = r'/Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir'\n",
    "        df3 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/synthetic_lhs_data.csv\")\n",
    "        df4 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/synthetic_rhs_data.csv\")\n",
    "        df1 = pd.read_excel(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/procore .xlsx\")\n",
    "        df2 = pd.read_excel(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/salesforce_leads.xlsx\")\n",
    "        mappings_dir = r'/Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir'\n",
    "        df3 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/synthetic_procore_data.csv\")\n",
    "        df4 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/synthetic_salesforce_data.csv\")\n",
    "    else:\n",
    "        print(\"Error: User not found\")\n",
    "        return None\n",
    "\n",
    "    return {\"df1\": df1, \"df2\": df2, \"mappings_dir\": mappings_dir, \"df3\": df3, \"df4\": df4}\n",
    "\n",
    "\n",
    "files = load_csv_files(user3)\n",
    "\n",
    "#Acces to the dataframes\n",
    "if files:\n",
    "    df1 = files[\"df1\"]\n",
    "    df2 = files[\"df2\"]\n",
    "    mappings_dir = files[\"mappings_dir\"]\n",
    "    df3 = files[\"df3\"]\n",
    "    df4 = files[\"df4\"]\n",
    "    print(\"Dataframes correctly loaded\")\n",
    "else:\n",
    "    print(\"No dataframes loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappings(mappings_dir):\n",
    "    mappings = {}\n",
    "    print(f\"Loading mappings from directory: {mappings_dir}\")\n",
    "\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            print(f\"Processing file: {filename} as database: {database_name}\")\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "                print(f\"Loaded mapping for {database_name}: {mappings[database_name]}\")\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "    print(\"Mappings loaded successfully:\", mappings)\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compatibilities(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_type(rhs_type, mappings):\n",
    "    if not mappings:\n",
    "        raise ValueError(\"Mappings dictionary is empty or not provided.\")\n",
    "    \n",
    "    rhs_type_lower = rhs_type.lower()\n",
    "    matching_databases = []\n",
    "    normalized_type = None\n",
    "\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "        normalized = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized:\n",
    "            normalized_type = normalized\n",
    "            matching_databases.append(db_name)\n",
    "\n",
    "    if matching_databases:\n",
    "        return normalized_type, matching_databases\n",
    "    \n",
    "    return rhs_type.upper(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_length(length):\n",
    "    \"\"\"Extracts precision and scale if decimal, else returns as integer.\"\"\"\n",
    "    if isinstance(length, str):\n",
    "        if ',' in length:  # Decimal format (e.g., \"10,2\")\n",
    "            try:\n",
    "                precision, scale = map(int, length.split(','))\n",
    "                return precision, scale\n",
    "            except ValueError:\n",
    "                return None  # Invalid decimal format\n",
    "        try:\n",
    "            return int(length), None  # Normal integer/string length\n",
    "        except ValueError:\n",
    "            return None  # Invalid format\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_length_compatible(lhs_length, rhs_length, check_length=True):\n",
    "    if not check_length:\n",
    "        return True\n",
    "\n",
    "    lhs_length = normalize_length(lhs_length)\n",
    "    rhs_length = normalize_length(rhs_length)\n",
    "\n",
    "    # If either is None (invalid), return False\n",
    "    if lhs_length is None or rhs_length is None:\n",
    "        return False  \n",
    "\n",
    "    # Case 1: Both are decimals (precision, scale)\n",
    "    if isinstance(lhs_length, tuple) and isinstance(rhs_length, tuple):\n",
    "        lhs_precision, lhs_scale = lhs_length\n",
    "        rhs_precision, rhs_scale = rhs_length\n",
    "        return lhs_precision <= rhs_precision and lhs_scale == rhs_scale\n",
    "    \n",
    "    # Case 2: One is decimal, and the other is not (Type Mismatch)\n",
    "    if isinstance(lhs_length, tuple) or isinstance(rhs_length, tuple):\n",
    "        return False  # Integer/String cannot be compared with Decimal\n",
    "\n",
    "    # Case 3: General Numeric & String Length Comparison (Integer/String Lengths)\n",
    "    return lhs_length <= rhs_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_similarity_score(lhs_length, rhs_length):\n",
    "    lhs_length = normalize_length(lhs_length)\n",
    "    rhs_length = normalize_length(rhs_length)\n",
    "    \n",
    "    if lhs_length is None or rhs_length is None:\n",
    "        return 0  # Return 0 if any length is invalid\n",
    "\n",
    "    # Case 1: Both are decimal (precision, scale)\n",
    "    if isinstance(lhs_length, tuple) and isinstance(rhs_length, tuple):\n",
    "        lhs_precision, lhs_scale = lhs_length\n",
    "        rhs_precision, rhs_scale = rhs_length\n",
    "        \n",
    "        # Calculate precision similarity\n",
    "        precision_diff = abs(lhs_precision - rhs_precision)\n",
    "        max_precision = max(lhs_precision, rhs_precision)\n",
    "        precision_score = 1 - (precision_diff / max_precision) if max_precision else 1\n",
    "\n",
    "        # Calculate scale similarity\n",
    "        scale_diff = abs(lhs_scale - rhs_scale)\n",
    "        max_scale = max(lhs_scale, rhs_scale)\n",
    "        scale_score = 1 - (scale_diff / max_scale) if max_scale else 1\n",
    "\n",
    "        # Average precision and scale similarity\n",
    "        return (precision_score + scale_score) / 2\n",
    "\n",
    "    # Case 2: One is decimal, the other is not (Type Mismatch)\n",
    "    if isinstance(lhs_length, tuple) or isinstance(rhs_length, tuple):\n",
    "        return 0  # Decimal vs Integer/String → No similarity\n",
    "\n",
    "    # Case 3: General Numeric & String Length Comparison\n",
    "    length_difference = abs(lhs_length - rhs_length)\n",
    "    max_length = max(lhs_length, rhs_length)\n",
    "    similarity_score = 1 - (length_difference / max_length) if max_length else 1\n",
    "\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Compatible data using Length and Data Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mappings from directory: /Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir\n",
      "Processing file: saphana_sql2023_map.json as database: saphana_sql2023_map\n",
      "Loaded mapping for saphana_sql2023_map: {'NVARCHAR': 'NVARCHAR', 'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'DATE': 'DATE', 'TIME': 'TIME', 'SECONDDATE': 'DATETIME2', 'TIMESTAMP': 'TIMESTAMP', 'DECIMAL': 'DECIMAL', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'FLOAT': 'FLOAT', 'REAL': 'REAL', 'DOUBLE': 'FLOAT', 'CLOB': 'VARCHAR(max)', 'NCLOB': 'NVARCHAR(max)', 'BLOB': 'VARBINARY(max)', 'BOOLEAN': 'BIT', 'ST_GEOMETRY': 'JSON', 'ST_POINT': 'JSON', 'JSON': 'JSON', 'GUID': 'UNIQUEIDENTIFIER', 'TINYINT': 'SMALLINT'}\n",
      "Processing file: sapmara_sql2023_map.json as database: sapmara_sql2023_map\n",
      "Loaded mapping for sapmara_sql2023_map: {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT', 'INT4': 'INT', 'CURR': 'DECIMAL', 'FLAG': 'BIT', 'UUID': 'UNIQUEIDENTIFIER', 'BIN': 'VARBINARY', 'JSON': 'JSON', 'TIMESTAMP': 'TIMESTAMP', 'TIME': 'TIME'}\n",
      "Processing file: pgsql_sql2023_map.json as database: pgsql_sql2023_map\n",
      "Loaded mapping for pgsql_sql2023_map: {'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'TEXT': 'VARCHAR(max)', 'BOOLEAN': 'BIT', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'NUMERIC': 'NUMERIC', 'DECIMAL': 'DECIMAL', 'REAL': 'REAL', 'DOUBLE PRECISION': 'FLOAT', 'DATE': 'DATE', 'TIME': 'TIME', 'TIMESTAMP': 'TIMESTAMP', 'TIMESTAMPTZ': 'TIMESTAMP WITH TIME ZONE', 'JSON': 'JSON', 'JSONB': 'JSON', 'UUID': 'UNIQUEIDENTIFIER', 'BYTEA': 'VARBINARY(max)', 'ARRAY': 'ARRAY', 'XML': 'XML', 'CITEXT': 'VARCHAR'}\n",
      "Processing file: salesforce_sql2023_map.json as database: salesforce_sql2023_map\n",
      "Loaded mapping for salesforce_sql2023_map: {'TEXT': 'VARCHAR', 'TEXTAREA': 'TEXT', 'EMAIL': 'VARCHAR', 'PHONE': 'VARCHAR', 'URL': 'VARCHAR', 'NUMBER': 'INT', 'CURRENCY': 'DECIMAL', 'PERCENT': 'DECIMAL', 'DATE': 'DATE', 'DATETIME': 'DATETIME', 'CHECKBOX': 'BOOLEAN', 'PICKLIST': 'ENUM', 'MULTISELECTPICKLIST': 'JSON', 'GEOLOCATION': 'GEOGRAPHY', 'AUTONUMBER': 'SERIAL', 'ENCRYPTEDTEXT': 'VARBINARY', 'RICHTEXTAREA': 'TEXT'}\n",
      "Mappings loaded successfully: {'saphana_sql2023_map': {'NVARCHAR': 'NVARCHAR', 'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'DATE': 'DATE', 'TIME': 'TIME', 'SECONDDATE': 'DATETIME2', 'TIMESTAMP': 'TIMESTAMP', 'DECIMAL': 'DECIMAL', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'FLOAT': 'FLOAT', 'REAL': 'REAL', 'DOUBLE': 'FLOAT', 'CLOB': 'VARCHAR(max)', 'NCLOB': 'NVARCHAR(max)', 'BLOB': 'VARBINARY(max)', 'BOOLEAN': 'BIT', 'ST_GEOMETRY': 'JSON', 'ST_POINT': 'JSON', 'JSON': 'JSON', 'GUID': 'UNIQUEIDENTIFIER', 'TINYINT': 'SMALLINT'}, 'sapmara_sql2023_map': {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT', 'INT4': 'INT', 'CURR': 'DECIMAL', 'FLAG': 'BIT', 'UUID': 'UNIQUEIDENTIFIER', 'BIN': 'VARBINARY', 'JSON': 'JSON', 'TIMESTAMP': 'TIMESTAMP', 'TIME': 'TIME'}, 'pgsql_sql2023_map': {'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'TEXT': 'VARCHAR(max)', 'BOOLEAN': 'BIT', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'NUMERIC': 'NUMERIC', 'DECIMAL': 'DECIMAL', 'REAL': 'REAL', 'DOUBLE PRECISION': 'FLOAT', 'DATE': 'DATE', 'TIME': 'TIME', 'TIMESTAMP': 'TIMESTAMP', 'TIMESTAMPTZ': 'TIMESTAMP WITH TIME ZONE', 'JSON': 'JSON', 'JSONB': 'JSON', 'UUID': 'UNIQUEIDENTIFIER', 'BYTEA': 'VARBINARY(max)', 'ARRAY': 'ARRAY', 'XML': 'XML', 'CITEXT': 'VARCHAR'}, 'salesforce_sql2023_map': {'TEXT': 'VARCHAR', 'TEXTAREA': 'TEXT', 'EMAIL': 'VARCHAR', 'PHONE': 'VARCHAR', 'URL': 'VARCHAR', 'NUMBER': 'INT', 'CURRENCY': 'DECIMAL', 'PERCENT': 'DECIMAL', 'DATE': 'DATE', 'DATETIME': 'DATETIME', 'CHECKBOX': 'BOOLEAN', 'PICKLIST': 'ENUM', 'MULTISELECTPICKLIST': 'JSON', 'GEOLOCATION': 'GEOGRAPHY', 'AUTONUMBER': 'SERIAL', 'ENCRYPTEDTEXT': 'VARBINARY', 'RICHTEXTAREA': 'TEXT'}}\n"
     ]
    }
   ],
   "source": [
    "type_mapping = load_mappings(mappings_dir)\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def filter_compatible_indices(df1, df2, type_mapping, compatibilities):\n",
    "    df1['Normalized_Type'], df1['Matching_Databases'] = zip(*df1['Data_Type'].apply(lambda x: normalize_data_type(x, type_mapping)))\n",
    "    df2['Normalized_Type'], df2['Matching_Databases'] = zip(*df2['Data_Type'].apply(lambda x: normalize_data_type(x, type_mapping)))\n",
    "    \n",
    "    compatible_index = defaultdict(list)\n",
    "    length_score = defaultdict(list)\n",
    "    \n",
    "    for idx1, row1 in df1.iterrows():\n",
    "        lhs_type = row1['Normalized_Type']\n",
    "        lhs_length = row1['Length']\n",
    "\n",
    "        for idx2, row2 in df2.iterrows():\n",
    "            rhs_type = row2['Normalized_Type']\n",
    "            rhs_length = row2['Length']\n",
    "            \n",
    "            if are_compatible(lhs_type, rhs_type, compatibilities) and is_length_compatible(lhs_length, rhs_length, check_length=True):\n",
    "                score = length_similarity_score(lhs_length, rhs_length)\n",
    "                compatible_index[idx1].append(idx2)\n",
    "                length_score[idx1].append(score)\n",
    "                \n",
    "    return compatible_index, length_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(column, batch_size, model):\n",
    "    embeddings = []\n",
    "    column = column.tolist()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(column), batch_size), desc=\"Encoding Batches\"):\n",
    "            batch = column[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(compatible_indices, df1, df2, column1, column2, model, calculated_similarities):\n",
    "    embeddings1 = batch_encode(df1[column1], batch_size=128, model=model)\n",
    "    embeddings2 = batch_encode(df2[column2], batch_size=128, model=model)\n",
    "\n",
    "    similarity_matrix = np.zeros((len(df1), len(df2)))\n",
    "\n",
    "    for idx1, compatible_idxs in compatible_indices.items():\n",
    "        for idx2 in compatible_idxs:\n",
    "            if (idx1, idx2) not in calculated_similarities:\n",
    "                similarity_score = util.cos_sim(embeddings1[idx1], embeddings2[idx2]).item()\n",
    "                similarity_matrix[idx1, idx2] = (similarity_score + 1) / 2  \n",
    "                calculated_similarities.add((idx1, idx2))\n",
    "\n",
    "    return csr_matrix(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native_type(value):\n",
    "    if isinstance(value, (np.integer, np.int64)):\n",
    "        return int(value)\n",
    "    elif isinstance(value, (np.floating, np.float64)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        return value.tolist()\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top similar Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top similar sentences based on similarity and compatibility\n",
    "import heapq\n",
    "\n",
    "\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, sparse_similarity_matrix, filtered_df1, filtered_df2, column1, column2, similarity_threshold, compatible_indices, length_scores):\n",
    "    # Convert the sparse similarity matrix row for the selected index to a dense array and flatten it\n",
    "    similarities = sparse_similarity_matrix[selected_index].toarray().flatten()\n",
    "    # Initialize a list to store matches\n",
    "    matches = []\n",
    "\n",
    "    # Check if the selected index is in the compatible indices dictionary\n",
    "    if selected_index in compatible_indices:  # data type and length compatibility\n",
    "        # Iterate over the compatible indices for the selected index\n",
    "        for idx, length_score in zip(compatible_indices[selected_index], length_scores[selected_index]):   \n",
    "            # Get the similarity score for the current index\n",
    "            similarity_score = similarities[idx]\n",
    "            # Skip if the similarity score is below the threshold\n",
    "            if similarity_score < similarity_threshold:\n",
    "                continue\n",
    "\n",
    "            # Retrieve relevant information from filtered_df2 for the current index\n",
    "            #rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_desc = filtered_df2.loc[idx, column2]\n",
    "            rhs_type = filtered_df2.loc[idx, 'Data_Type']\n",
    "            rhs_length = filtered_df2.loc[idx, 'Length']\n",
    "            matching_databases = filtered_df2.loc[idx, 'Matching_Databases']\n",
    "            \n",
    "            # Append the match information to the matches list\n",
    "            matches.append({\n",
    "                \"rank\": None,\n",
    "                \"similarity_score\": float(similarity_score),\n",
    "                \"rhs_index\": int(filtered_df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"matching_databases\": matching_databases if matching_databases else [\"Unknown\"],\n",
    "                \"rhs_length\": convert_to_native_type(rhs_length),\n",
    "                \"length_score\": float(length_score),\n",
    "                \"compatibility\": \"Compatible\"\n",
    "            })\n",
    "\n",
    "    # Sort the matches by similarity score in descending order and keep only the top N matches\n",
    "    matches = sorted(matches, key=lambda x: (x[\"similarity_score\"], x[\"length_score\"]), reverse=True)[:top_n]\n",
    "        \n",
    "    #matches = sorted(matches, key=lambda x: x[\"similarity_score\"], reverse=True)[:top_n]\n",
    "\n",
    "    # Assign ranks to the matches\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "\n",
    "    # If no matches are found, add a message indicating no compatible matches\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Retrieve relevant information from filtered_df1 for the selected index\n",
    "    lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = filtered_df1.loc[selected_index, column1]\n",
    "    lhs_type = filtered_df1.loc[selected_index, 'Data_Type']\n",
    "    lhs_length = filtered_df1.loc[selected_index, 'Length']\n",
    "\n",
    "    # Create the result dictionary containing the LHS field information and matches\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"lhs_field_length\": convert_to_native_type(lhs_length),\n",
    "        \"matches\": matches\n",
    "    }\n",
    "\n",
    "    # Return the result wrapped in a dictionary with a \"results\" key\n",
    "    return {\"results\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mappings from directory: /Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir\n",
      "Processing file: saphana_sql2023_map.json as database: saphana_sql2023_map\n",
      "Loaded mapping for saphana_sql2023_map: {'NVARCHAR': 'NVARCHAR', 'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'DATE': 'DATE', 'TIME': 'TIME', 'SECONDDATE': 'DATETIME2', 'TIMESTAMP': 'TIMESTAMP', 'DECIMAL': 'DECIMAL', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'FLOAT': 'FLOAT', 'REAL': 'REAL', 'DOUBLE': 'FLOAT', 'CLOB': 'VARCHAR(max)', 'NCLOB': 'NVARCHAR(max)', 'BLOB': 'VARBINARY(max)', 'BOOLEAN': 'BIT', 'ST_GEOMETRY': 'JSON', 'ST_POINT': 'JSON', 'JSON': 'JSON', 'GUID': 'UNIQUEIDENTIFIER', 'TINYINT': 'SMALLINT'}\n",
      "Processing file: sapmara_sql2023_map.json as database: sapmara_sql2023_map\n",
      "Loaded mapping for sapmara_sql2023_map: {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT', 'INT4': 'INT', 'CURR': 'DECIMAL', 'FLAG': 'BIT', 'UUID': 'UNIQUEIDENTIFIER', 'BIN': 'VARBINARY', 'JSON': 'JSON', 'TIMESTAMP': 'TIMESTAMP', 'TIME': 'TIME'}\n",
      "Processing file: pgsql_sql2023_map.json as database: pgsql_sql2023_map\n",
      "Loaded mapping for pgsql_sql2023_map: {'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'TEXT': 'VARCHAR(max)', 'BOOLEAN': 'BIT', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'NUMERIC': 'NUMERIC', 'DECIMAL': 'DECIMAL', 'REAL': 'REAL', 'DOUBLE PRECISION': 'FLOAT', 'DATE': 'DATE', 'TIME': 'TIME', 'TIMESTAMP': 'TIMESTAMP', 'TIMESTAMPTZ': 'TIMESTAMP WITH TIME ZONE', 'JSON': 'JSON', 'JSONB': 'JSON', 'UUID': 'UNIQUEIDENTIFIER', 'BYTEA': 'VARBINARY(max)', 'ARRAY': 'ARRAY', 'XML': 'XML', 'CITEXT': 'VARCHAR'}\n",
      "Processing file: salesforce_sql2023_map.json as database: salesforce_sql2023_map\n",
      "Loaded mapping for salesforce_sql2023_map: {'TEXT': 'VARCHAR', 'TEXTAREA': 'TEXT', 'EMAIL': 'VARCHAR', 'PHONE': 'VARCHAR', 'URL': 'VARCHAR', 'NUMBER': 'INT', 'CURRENCY': 'DECIMAL', 'PERCENT': 'DECIMAL', 'DATE': 'DATE', 'DATETIME': 'DATETIME', 'CHECKBOX': 'BOOLEAN', 'PICKLIST': 'ENUM', 'MULTISELECTPICKLIST': 'JSON', 'GEOLOCATION': 'GEOGRAPHY', 'AUTONUMBER': 'SERIAL', 'ENCRYPTEDTEXT': 'VARBINARY', 'RICHTEXTAREA': 'TEXT'}\n",
      "Mappings loaded successfully: {'saphana_sql2023_map': {'NVARCHAR': 'NVARCHAR', 'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'DATE': 'DATE', 'TIME': 'TIME', 'SECONDDATE': 'DATETIME2', 'TIMESTAMP': 'TIMESTAMP', 'DECIMAL': 'DECIMAL', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'FLOAT': 'FLOAT', 'REAL': 'REAL', 'DOUBLE': 'FLOAT', 'CLOB': 'VARCHAR(max)', 'NCLOB': 'NVARCHAR(max)', 'BLOB': 'VARBINARY(max)', 'BOOLEAN': 'BIT', 'ST_GEOMETRY': 'JSON', 'ST_POINT': 'JSON', 'JSON': 'JSON', 'GUID': 'UNIQUEIDENTIFIER', 'TINYINT': 'SMALLINT'}, 'sapmara_sql2023_map': {'CLNT': 'VARCHAR', 'CHAR': 'CHAR', 'DATS': 'DATE', 'UNIT': 'VARCHAR', 'NUMC': 'NUMERIC', 'QUAN': 'FLOAT', 'DEC': 'DECIMAL', 'INT2': 'SMALLINT', 'INT4': 'INT', 'CURR': 'DECIMAL', 'FLAG': 'BIT', 'UUID': 'UNIQUEIDENTIFIER', 'BIN': 'VARBINARY', 'JSON': 'JSON', 'TIMESTAMP': 'TIMESTAMP', 'TIME': 'TIME'}, 'pgsql_sql2023_map': {'VARCHAR': 'VARCHAR', 'CHAR': 'CHAR', 'TEXT': 'VARCHAR(max)', 'BOOLEAN': 'BIT', 'SMALLINT': 'SMALLINT', 'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'NUMERIC': 'NUMERIC', 'DECIMAL': 'DECIMAL', 'REAL': 'REAL', 'DOUBLE PRECISION': 'FLOAT', 'DATE': 'DATE', 'TIME': 'TIME', 'TIMESTAMP': 'TIMESTAMP', 'TIMESTAMPTZ': 'TIMESTAMP WITH TIME ZONE', 'JSON': 'JSON', 'JSONB': 'JSON', 'UUID': 'UNIQUEIDENTIFIER', 'BYTEA': 'VARBINARY(max)', 'ARRAY': 'ARRAY', 'XML': 'XML', 'CITEXT': 'VARCHAR'}, 'salesforce_sql2023_map': {'TEXT': 'VARCHAR', 'TEXTAREA': 'TEXT', 'EMAIL': 'VARCHAR', 'PHONE': 'VARCHAR', 'URL': 'VARCHAR', 'NUMBER': 'INT', 'CURRENCY': 'DECIMAL', 'PERCENT': 'DECIMAL', 'DATE': 'DATE', 'DATETIME': 'DATETIME', 'CHECKBOX': 'BOOLEAN', 'PICKLIST': 'ENUM', 'MULTISELECTPICKLIST': 'JSON', 'GEOLOCATION': 'GEOGRAPHY', 'AUTONUMBER': 'SERIAL', 'ENCRYPTEDTEXT': 'VARBINARY', 'RICHTEXTAREA': 'TEXT'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Batches: 100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "Encoding Batches: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load mappings and compatibilities\n",
    "# Load the type mappings from the mappings directory\n",
    "type_mapping = load_mappings(mappings_dir)\n",
    "# Define the path to the compatibilities file\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "# Load the compatibilities from the file\n",
    "compatibilities = load_compatibilities(compatibilities_file)\n",
    "\n",
    "# Filter compatible data\n",
    "# Filter the dataframes df1 and df2 to find compatible indices based on type and length\n",
    "compatible_indices, length_score = filter_compatible_indices(df1, df2, type_mapping, compatibilities)\n",
    "\n",
    "# Convert columns to string type\n",
    "# Define the column name for descriptions in df1 and df2\n",
    "column1 = 'Description'\n",
    "# Convert the 'Description' column in df1 to string type\n",
    "df1[column1] = df1[column1].astype(str)\n",
    "column2 = 'Description'\n",
    "# Convert the 'Description' column in df2 to string type\n",
    "df2[column2] = df2[column2].astype(str)\n",
    "\n",
    "# Load the model\n",
    "# Load the SentenceTransformer model for encoding text data\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "\n",
    "\n",
    "# Calculate similarity matrix\n",
    "\n",
    "calculated_similarities = set()\n",
    "\n",
    "# Compute similarity matrix\n",
    "# Compute the cosine similarity matrix between the embeddings of df1 and df2\n",
    "sparse_similarity_matrix = calculate_similarities(compatible_indices, df1, df2, column1, column2, model, calculated_similarities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = sparse_similarity_matrix.max()\n",
    "max_index = sparse_similarity_matrix.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor máximo es 0.6456390917301178 en la posición (3, 6)\n"
     ]
    }
   ],
   "source": [
    "max_row, max_col = np.unravel_index(max_index, sparse_similarity_matrix.shape)\n",
    "print(f\"El valor máximo es {max_value} en la posición ({max_row}, {max_col})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"lhs_field_index\": 3,\n",
      "      \"lhs_field_name\": \"Budget\",\n",
      "      \"lhs_field_description\": \"Budget allocated for the project\",\n",
      "      \"lhs_field_data_type\": \"Decimal\",\n",
      "      \"lhs_field_length\": \"10,2\",\n",
      "      \"matches\": [\n",
      "        {\n",
      "          \"rank\": 1,\n",
      "          \"similarity_score\": 0.6456390917301178,\n",
      "          \"rhs_index\": 6,\n",
      "          \"rhs_field_name\": \"Revenue\",\n",
      "          \"rhs_field_desc\": \"The total annual revenue of the account\",\n",
      "          \"rhs_data_type\": \"Decimal\",\n",
      "          \"matching_databases\": [\n",
      "            \"saphana_sql2023_map\",\n",
      "            \"pgsql_sql2023_map\"\n",
      "          ],\n",
      "          \"rhs_length\": \"15,2\",\n",
      "          \"length_score\": 0.8333333333333334,\n",
      "          \"compatibility\": \"Compatible\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# User Input with Validations\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "#similarity_threshold = float(input(\"Enter the similarity score threshold (0.0 - 1.0): \"))\n",
    "similarity_threshold = 0.5\n",
    "# Retrieve top similar sentences\n",
    "\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, sparse_similarity_matrix, df1, df2, column1, column2, similarity_threshold, compatible_indices, length_score)\n",
    "print(json.dumps(output_json, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "with open(\"output_DataType_Length_Index_1.json\", \"w\") as json_file:     \n",
    "    json.dump(output_json, json_file, indent=4) \n",
    "    print(\"JSON file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data_Type\n",
       "String     5\n",
       "DATE       2\n",
       "Decimal    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Data_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data_Type\n",
       "String     10\n",
       "Date        2\n",
       "Decimal     1\n",
       "Integer     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Data_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert series to numeric values\n",
    "def convert_numeric_values(series):\n",
    "    try:\n",
    "        # Attempt to convert the series to numeric values, coercing errors to NaN\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "    except Exception as e:\n",
    "        # Print an error message if conversion fails\n",
    "        print(f\"Conversion error: {e}\")\n",
    "        return series\n",
    "\n",
    "# Function to compute statistical similarity between two columns\n",
    "def compute_statistical_similarity(lhs_col, rhs_col, dtype_category):\n",
    "    \"\"\"\n",
    "    Determines if two data distributions are statistically similar based on their type.\n",
    "    \"\"\"\n",
    "    # Initialize similarity score to 0\n",
    "    similarity_score = 0\n",
    "    \n",
    "    # Drop NaN values from both columns\n",
    "    lhs_values = lhs_col.dropna()\n",
    "    rhs_values = rhs_col.dropna()\n",
    "    \n",
    "    # Check if the data type category is numeric\n",
    "    if dtype_category in [\"INT\", \"INTEGER\", \"BIGINT\", \"SMALLINT\", \"FLOAT\", \"DOUBLE\", \"REAL\", \"NUMERIC\", \"DECIMAL\"]:\n",
    "        # Convert columns to numeric values\n",
    "        lhs_values = convert_numeric_values(lhs_col.dropna())\n",
    "        rhs_values = convert_numeric_values(rhs_col.dropna())\n",
    "        \n",
    "        # Compute Wasserstein distance and Kolmogorov-Smirnov statistic if both columns have values\n",
    "        if len(lhs_values) > 0 and len(rhs_values) > 0:\n",
    "            wasserstein_dist = stats.wasserstein_distance(lhs_values, rhs_values)\n",
    "            ks_stat, ks_p_value = stats.ks_2samp(lhs_values, rhs_values)\n",
    "            # Calculate similarity score based on Wasserstein distance and KS statistic\n",
    "            similarity_score = 1 / (1 + wasserstein_dist) if wasserstein_dist > 0 else 1\n",
    "            similarity_score = min(similarity_score, 1 - ks_stat)  # Normalize\n",
    "        else:\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    \n",
    "    # Check if the data type category is categorical\n",
    "    elif dtype_category in [\"CHAR\", \"VARCHAR\", \"TEXT\", \"STRING\", \"JSON\"]:\n",
    "        # Convert columns to unique string values\n",
    "        lhs_values = lhs_col.dropna().astype(str).unique()\n",
    "        rhs_values = rhs_col.dropna().astype(str).unique()\n",
    "        \n",
    "        # Compute Jaccard similarity if both columns have values\n",
    "        if len(lhs_values) > 0 and len(rhs_values) > 0:\n",
    "            vectorizer = CountVectorizer(preprocessor=lambda x: x, binary=True)\n",
    "            matrix = vectorizer.fit_transform([' '.join(lhs_values), ' '.join(rhs_values)])\n",
    "            similarity_score = 1 - jaccard(matrix.toarray()[0], matrix.toarray()[1])\n",
    "        else:\n",
    "            print(\"something happen\")\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    elif dtype_category in [\"DATE\", \"TIMESTAMP\", \"TIME\", \"DATETIME\"]:\n",
    "\n",
    "        # Convert columns to datetime values\n",
    "        lhs_values = pd.to_datetime(lhs_col, errors='coerce')\n",
    "        rhs_values = pd.to_datetime(rhs_col, errors='coerce')\n",
    "        \n",
    "        # Drop NaN values from both columns\n",
    "        lhs_values = lhs_values.dropna()\n",
    "        rhs_values = rhs_values.dropna()\n",
    "        \n",
    "        # Compute similarity score based on min and max date values\n",
    "        if not lhs_values.empty and not rhs_values.empty:\n",
    "            min_date_col1 = lhs_values.min()\n",
    "            max_date_col1 = lhs_values.max()\n",
    "            min_date_col2 = rhs_values.min()\n",
    "            max_date_col2 = rhs_values.max()\n",
    "            \n",
    "            # Calculate similarity score based on date range overlap\n",
    "            similarity_score = 1 - (abs((min_date_col1 - min_date_col2).days) + abs((max_date_col1 - max_date_col2).days)) / 365\n",
    "        else:\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    elif dtype_category in [\"BOOLEAN\"]:\n",
    "        # Convert columns to boolean values\n",
    "        lhs_values = lhs_col.dropna().astype(bool)\n",
    "        rhs_values = rhs_col.dropna().astype(bool)\n",
    "        \n",
    "        # Compute similarity score based on Jaccard similarity of boolean values\n",
    "        similarity_score = jaccard_score(lhs_values, rhs_values)\n",
    "    else:\n",
    "        print(\"NOT WORK FOR THIS DATA TYPE\")\n",
    "        similarity_score = 0  # Incompatible types\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "# Function to truncate percentage values to two decimal places\n",
    "def truncate_percentage(value):\n",
    "    return f\"{value:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top similar sentences based on similarity and statistical compatibility\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, sparse_similarity_matrix, filtered_df1, filtered_df2, column1, column2, similarity_threshold, statistical_threshold, compatible_indices, length_scores):\n",
    "    # Convert the sparse similarity matrix row for the selected index to a dense array and flatten it\n",
    "    similarities = sparse_similarity_matrix[selected_index].toarray().flatten()\n",
    "    # Initialize a list to store matches\n",
    "    matches = []\n",
    "\n",
    "    # Check if the selected index is in the compatible indices dictionary\n",
    "    if selected_index in compatible_indices:\n",
    "        # Iterate over the compatible indices for the selected index\n",
    "        for idx, length_score in zip(compatible_indices[selected_index], length_scores[selected_index]):  \n",
    "            # Get the similarity score for the current index\n",
    "            similarity_score = similarities[idx]\n",
    "            # Skip if the similarity score is below the threshold\n",
    "            if similarity_score < similarity_threshold:\n",
    "                continue\n",
    "\n",
    "            # Retrieve relevant information from filtered_df2 for the current index\n",
    "            rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_desc = filtered_df2.loc[idx, column2]\n",
    "            rhs_type = filtered_df2.loc[idx, 'Data_Type']\n",
    "            normalized_rhs_type = filtered_df2.loc[idx, 'Normalized_Type']\n",
    "            rhs_length = filtered_df2.loc[idx, 'Length']\n",
    "            matching_databases = filtered_df2.loc[idx, 'Matching_Databases']\n",
    "            \n",
    "            # Retrieve the field name from filtered_df1 for the selected index\n",
    "            lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "\n",
    "            # Check if the lhs_field and rhs_field exist in df3 and df4 respectively\n",
    "            if lhs_field in df3.columns and rhs_field in df4.columns:\n",
    "                # Retrieve the columns from df3 and df4\n",
    "                lhs_col = df3[lhs_field]\n",
    "                rhs_col = df4[rhs_field]\n",
    "                # Compute the statistical similarity between the columns\n",
    "                stat_similarity = compute_statistical_similarity(lhs_col, rhs_col, normalized_rhs_type)\n",
    "            else:\n",
    "                stat_similarity = 0  # No valid comparison\n",
    "                \n",
    "            # Convert the statistical similarity to a percentage\n",
    "            stat_similarity_percentage = truncate_percentage(stat_similarity * 100)\n",
    "            \n",
    "            # Skip if the statistical similarity is below the threshold\n",
    "            if stat_similarity < statistical_threshold:\n",
    "                continue\n",
    "            \n",
    "                \n",
    "            # Append the match information to the matches list\n",
    "            matches.append({\n",
    "                \"rank\": None,\n",
    "                \"similarity_score\": float(similarity_score),\n",
    "                \"rhs_index\": int(filtered_df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"matching_databases\": matching_databases if matching_databases else [\"Unknown\"],\n",
    "                \"rhs_length\": convert_to_native_type(rhs_length),\n",
    "                \"length_score\": float(length_score),\n",
    "                \"statistical_similarity\": stat_similarity_percentage  \n",
    "            })\n",
    "\n",
    "    # Sort the matches by similarity score in descending order and keep only the top N matches\n",
    "    #matches = sorted(matches, key=lambda x: x[\"similarity_score\"], reverse=True)[:top_n]\n",
    "    matches = sorted(matches, key=lambda x: (x[\"similarity_score\"],x[\"length_score\"], x[\"statistical_similarity\"]), reverse=True)[:top_n]\n",
    "\n",
    "    # Assign ranks to the matches\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "\n",
    "    # If no matches are found, add a message indicating no compatible matches\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Retrieve relevant information from filtered_df1 for the selected index\n",
    "    lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = filtered_df1.loc[selected_index, column1]\n",
    "    lhs_type = filtered_df1.loc[selected_index, 'Data_Type']\n",
    "    lhs_length = filtered_df1.loc[selected_index, 'Length']\n",
    "\n",
    "    # Create the result dictionary containing the LHS field information and matches\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"lhs_field_length\": convert_to_native_type(lhs_length),\n",
    "        \"matches\": matches\n",
    "    }\n",
    "\n",
    "    # Return the result wrapped in a dictionary with a \"results\" key\n",
    "    return {\"results\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"lhs_field_index\": 3,\n",
      "      \"lhs_field_name\": \"Budget\",\n",
      "      \"lhs_field_description\": \"Budget allocated for the project\",\n",
      "      \"lhs_field_data_type\": \"Decimal\",\n",
      "      \"lhs_field_length\": \"10,2\",\n",
      "      \"matches\": [\n",
      "        {\n",
      "          \"rank\": 1,\n",
      "          \"similarity_score\": 0.6456390917301178,\n",
      "          \"rhs_index\": 6,\n",
      "          \"rhs_field_name\": \"Revenue\",\n",
      "          \"rhs_field_desc\": \"The total annual revenue of the account\",\n",
      "          \"rhs_data_type\": \"Decimal\",\n",
      "          \"matching_databases\": [\n",
      "            \"saphana_sql2023_map\",\n",
      "            \"pgsql_sql2023_map\"\n",
      "          ],\n",
      "          \"rhs_length\": \"15,2\",\n",
      "          \"length_score\": 0.8333333333333334,\n",
      "          \"statistical_similarity\": \"1.20\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# User Input with Validations\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "#similarity_threshold = float(input(\"Enter the similarity score threshold (0.0 - 1.0): \"))\n",
    "#statistical_threshold = float(input(\"Enter the statistical similarity score threshold (0.0 - 1.0): \"))\n",
    "\n",
    "similarity_threshold = 0.5\n",
    "statistical_threshold = 0.0\n",
    "\n",
    "# Retrieve top similar sentences\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, sparse_similarity_matrix, df1, df2, column1, column2, similarity_threshold,statistical_threshold, compatible_indices, length_score)\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "with open(\"output_statistics_compatibility_Index_1.json\", \"w\") as json_file:     \n",
    "    json.dump(output_json, json_file, indent=4) \n",
    "    print(\"JSON file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Null Values",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Distinct Values",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Non-Distinct Values",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Most Common Value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Mean Length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Min Length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Max Length",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b7fb3fc0-213d-4280-ace3-17feb541ebb2",
       "rows": [
        [
         "0",
         "Budget",
         "0",
         "775",
         "13",
         "2749.818",
         "7.775380710659898",
         "4",
         "8"
        ],
        [
         "1",
         "Revenue",
         "0",
         "1287",
         "178",
         "268.994",
         "7.785665529010239",
         "5",
         "8"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Null Values</th>\n",
       "      <th>Distinct Values</th>\n",
       "      <th>Non-Distinct Values</th>\n",
       "      <th>Most Common Value</th>\n",
       "      <th>Mean Length</th>\n",
       "      <th>Min Length</th>\n",
       "      <th>Max Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Budget</td>\n",
       "      <td>0</td>\n",
       "      <td>775</td>\n",
       "      <td>13</td>\n",
       "      <td>2749.818</td>\n",
       "      <td>7.775381</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revenue</td>\n",
       "      <td>0</td>\n",
       "      <td>1287</td>\n",
       "      <td>178</td>\n",
       "      <td>268.994</td>\n",
       "      <td>7.785666</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column  Null Values  Distinct Values  Non-Distinct Values  \\\n",
       "0   Budget            0              775                   13   \n",
       "1  Revenue            0             1287                  178   \n",
       "\n",
       "   Most Common Value  Mean Length  Min Length  Max Length  \n",
       "0           2749.818     7.775381           4           8  \n",
       "1            268.994     7.785666           5           8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Data Type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Top Patterns",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Inferred Data Domain",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e0379109-d4dc-4705-95d9-9b01f29892dd",
       "rows": [
        [
         "0",
         "Budget",
         "float64",
         "[]",
         "Numerical"
        ],
        [
         "1",
         "Revenue",
         "float64",
         "[]",
         "Numerical"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Top Patterns</th>\n",
       "      <th>Inferred Data Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Budget</td>\n",
       "      <td>float64</td>\n",
       "      <td>[]</td>\n",
       "      <td>Numerical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revenue</td>\n",
       "      <td>float64</td>\n",
       "      <td>[]</td>\n",
       "      <td>Numerical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column Data Type Top Patterns Inferred Data Domain\n",
       "0   Budget   float64           []            Numerical\n",
       "1  Revenue   float64           []            Numerical"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import IPython.display as display\n",
    "\n",
    "\n",
    "# Load data into DataFrames\n",
    "lhs_df = df3\n",
    "rhs_df = df4\n",
    "\n",
    "# Define columns of interest\n",
    "lhs_column = output_json[\"results\"][0][\"lhs_field_name\"]\n",
    "if \"message\" not in output_json[\"results\"][0][\"matches\"][0]:\n",
    "    rhs_columns = [match[\"rhs_field_name\"] for match in output_json[\"results\"][0][\"matches\"]]\n",
    "else:\n",
    "    rhs_columns = []\n",
    "\n",
    "# Extract relevant columns\n",
    "lhs_data = lhs_df[[lhs_column]]\n",
    "rhs_data = rhs_df[rhs_columns]\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = pd.DataFrame(columns=[\"Column\", \"Null Values\", \"Distinct Values\", \"Non-Distinct Values\", \"Most Common Value\", \"Mean Length\", \"Min Length\", \"Max Length\"])\n",
    "\n",
    "for col in [lhs_column] + rhs_columns:\n",
    "    null_values = lhs_df[col].isnull().sum() if col == lhs_column else rhs_df[col].isnull().sum()\n",
    "    distinct_values = lhs_df[col].nunique() if col == lhs_column else rhs_df[col].nunique()\n",
    "    total_values = len(lhs_df[col]) if col == lhs_column else len(rhs_df[col])\n",
    "    non_distinct_values = total_values - distinct_values\n",
    "    most_common_value = lhs_df[col].mode()[0] if col == lhs_column else rhs_df[col].mode()[0]\n",
    "    \n",
    "    mean_length = lhs_df[col].astype(str).str.len().mean() if col == lhs_column else rhs_df[col].astype(str).str.len().mean()\n",
    "    min_length = lhs_df[col].astype(str).str.len().min() if col == lhs_column else rhs_df[col].astype(str).str.len().min()\n",
    "    max_length = lhs_df[col].astype(str).str.len().max() if col == lhs_column else rhs_df[col].astype(str).str.len().max()\n",
    "    \n",
    "    summary_stats.loc[len(summary_stats)] = [col, null_values, distinct_values, non_distinct_values, most_common_value, mean_length, min_length, max_length]\n",
    "\n",
    "# Display summary statistics\n",
    "display.display(summary_stats)\n",
    "\n",
    "# Generate detailed view with inferred patterns and data types\n",
    "detailed_view = pd.DataFrame(columns=[\"Column\", \"Data Type\", \"Top Patterns\", \"Inferred Data Domain\"])\n",
    "\n",
    "for col in [lhs_column] + rhs_columns:\n",
    "    data_type = lhs_df[col].dtype if col == lhs_column else rhs_df[col].dtype\n",
    "    inferred_patterns = lhs_df[col].astype(str).str.extract(r'([A-Za-z]+)')[0].dropna().unique() if col == lhs_column else rhs_df[col].astype(str).str.extract(r'([A-Za-z]+)')[0].dropna().unique()\n",
    "    inferred_data_domain = \"Categorical\" if data_type == 'object' else \"Numerical\"\n",
    "    \n",
    "    detailed_view.loc[len(detailed_view)] = [col, data_type, inferred_patterns, inferred_data_domain]\n",
    "\n",
    "# Display detailed view\n",
    "display.display(detailed_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
