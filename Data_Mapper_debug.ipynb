{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install sentence-transformers\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install --upgrade transformers sentence-transformers\n",
    "%pip install pybloom_live\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance, ks_2samp\n",
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import jaccard\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from openpyxl import Workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Injection (Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user1 = \"Krithika\"\n",
    "user2 = \"Yasvanth\"\n",
    "user3 = \"Rebeca\"\n",
    "\n",
    "\n",
    "def load_csv_files(user):\n",
    "    if user == \"Krithika\":\n",
    "        df1 = pd.read_csv(r\"C:/Users/Krithika.Patali/DataMapper/LHS_3.csv\")\n",
    "        df2 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/RHS 3.csv\")\n",
    "        mappings_dir = r'C:\\Users\\Krithika.Patali\\DataMapper\\Mappings'\n",
    "        df3 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/synthetic_lhs_data.csv\")\n",
    "        df4 = pd.read_csv(r\"C:/Users/Krithika.Patali/Downloads/synthetic_rhs_data.csv\")\n",
    "    elif user == \"Yasvanth\":\n",
    "        df1 = pd.read_excel(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\procore .xlsx\")\n",
    "        df2 = pd.read_excel(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\salesforce_leads.xlsx\")\n",
    "        mappings_dir = r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\VSC\\DataMapper\\mappings_dir\"\n",
    "        df3 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\synthetic_procore_data.csv\")\n",
    "        df4 = pd.read_csv(r\"C:\\Users\\Yasvanth.Pamidi\\OneDrive - ENCORA\\Desktop\\DataMap\\Trugrid\\synthetic_salesforce_data.csv\")\n",
    "    elif user == \"Rebeca\":\n",
    "        df1 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/LHS 2.csv\")\n",
    "        f2 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/RHS 3.csv\")\n",
    "        mappings_dir = r'/Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir'\n",
    "        df3 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/synthetic_lhs_data.csv\")\n",
    "        df4 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/synthetic_rhs_data.csv\")\n",
    "        df1 = pd.read_excel(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/procore .xlsx\")\n",
    "        df2 = pd.read_excel(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/salesforce_leads.xlsx\")\n",
    "        mappings_dir = r'/Users/rebeca.mendoza/Desktop/Data_Mapper/mappings_dir'\n",
    "        df3 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/synthetic_procore_data.csv\")\n",
    "        df4 = pd.read_csv(r\"/Users/rebeca.mendoza/Desktop/Data_Mapper/Ahkila/synthetic_salesforce_data.csv\")\n",
    "    else:\n",
    "        print(\"Error: User not found\")\n",
    "        return None\n",
    "\n",
    "    return {\"df1\": df1, \"df2\": df2, \"mappings_dir\": mappings_dir, \"df3\": df3, \"df4\": df4}\n",
    "\n",
    "\n",
    "files = load_csv_files(user2)\n",
    "\n",
    "#Acces to the dataframes\n",
    "if files:\n",
    "    df1 = files[\"df1\"]\n",
    "    df2 = files[\"df2\"]\n",
    "    mappings_dir = files[\"mappings_dir\"]\n",
    "    df3 = files[\"df3\"]\n",
    "    df4 = files[\"df4\"]\n",
    "    print(\"Dataframes correctly loaded\")\n",
    "else:\n",
    "    print(\"No dataframes loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappings(mappings_dir):\n",
    "    mappings = {}\n",
    "   # print(f\"Loading mappings from directory: {mappings_dir}\")\n",
    "\n",
    "    if not os.path.exists(mappings_dir):\n",
    "        raise FileNotFoundError(f\"Mappings directory '{mappings_dir}' does not exist.\")\n",
    "    \n",
    "    for filename in os.listdir(mappings_dir):\n",
    "        if filename.endswith(\".json\") and filename != \"compatibilities.json\":\n",
    "            database_name = os.path.splitext(filename)[0]\n",
    "            #print(f\"Processing file: {filename} as database: {database_name}\")\n",
    "            with open(os.path.join(mappings_dir, filename), \"r\") as file:\n",
    "                mappings[database_name] = json.load(file)\n",
    "                #print(f\"Loaded mapping for {database_name}: {mappings[database_name]}\")\n",
    "    \n",
    "    if not mappings:\n",
    "        raise ValueError(f\"No mapping files found in '{mappings_dir}'.\")\n",
    "    \n",
    "   # print(\"Mappings loaded successfully:\", mappings)\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compatibilities(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_type(rhs_type, mappings):\n",
    "    if not mappings:\n",
    "        raise ValueError(\"Mappings dictionary is empty or not provided.\")\n",
    "    \n",
    "    rhs_type_lower = rhs_type.lower()\n",
    "    matching_databases = []\n",
    "    normalized_type = None\n",
    "\n",
    "    for db_name, db_mapping in mappings.items():\n",
    "        normalized = {k.lower(): v for k, v in db_mapping.items()}.get(rhs_type_lower)\n",
    "        if normalized:\n",
    "            normalized_type = normalized\n",
    "            matching_databases.append(db_name)\n",
    "\n",
    "    if matching_databases:\n",
    "        return normalized_type, matching_databases\n",
    "    \n",
    "    return rhs_type.upper(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "    lhs_type = lhs_type.upper()\n",
    "    rhs_type = rhs_type.upper()\n",
    "    return lhs_type == rhs_type or rhs_type in compatibilities.get(lhs_type, [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_length(length):\n",
    "    \"\"\"Extracts precision and scale if decimal, else returns as integer.\"\"\"\n",
    "    print(f\"Normalizing length: {length}\")\n",
    "\n",
    "    # If the input is already a tuple (precision, scale), return it directly\n",
    "    if isinstance(length, tuple) and len(length) == 2:\n",
    "        print(f\"  - Already a decimal tuple: {length}\")\n",
    "        return length  # No need to change\n",
    "\n",
    "    # If the input is already an integer, return it directly\n",
    "    if isinstance(length, int):\n",
    "        print(f\"  - Already an integer length: {length}\")\n",
    "        return length  \n",
    "\n",
    "    # If the input is a string, process it\n",
    "    if isinstance(length, str):\n",
    "        if ',' in length:  # Decimal format (e.g., \"10,2\")\n",
    "            try:\n",
    "                precision, scale = map(int, length.split(','))\n",
    "                print(f\"  - Parsed as decimal (Precision: {precision}, Scale: {scale})\")\n",
    "                return precision, scale\n",
    "            except ValueError:\n",
    "                print(\"  - Error: Invalid decimal format.\")\n",
    "                return None  # Invalid decimal format\n",
    "        \n",
    "        try:\n",
    "            normalized_length = int(length)\n",
    "            print(f\"  - Parsed as integer length: {normalized_length}\")\n",
    "            return normalized_length  # Return integer directly\n",
    "        except ValueError:\n",
    "            print(\"  - Error: Invalid format, cannot convert to integer.\")\n",
    "            return None  # Invalid format\n",
    "\n",
    "    # If it reaches here, the input is invalid\n",
    "    print(\"  - Error: Unsupported length format.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_length_compatible(lhs_length, rhs_length, check_length=True):\n",
    "    print(f\"Checking length compatibility: LHS = {lhs_length}, RHS = {rhs_length}, check_length = {check_length}\")\n",
    "\n",
    "    if not check_length:\n",
    "        print(\"  - Length check disabled. Returning True.\")\n",
    "        return True\n",
    "\n",
    "    lhs_length = normalize_length(lhs_length)\n",
    "    rhs_length = normalize_length(rhs_length)\n",
    "\n",
    "    print(f\"  - Normalized lengths: LHS = {lhs_length}, RHS = {rhs_length}\")\n",
    "\n",
    "    # If either length is invalid, return False\n",
    "    if lhs_length is None or rhs_length is None:\n",
    "        print(\"  - Error: One or both lengths are invalid. Returning False.\")\n",
    "        return False  \n",
    "\n",
    "    # Case 1: Both are decimals (precision, scale)\n",
    "    if isinstance(lhs_length, tuple) and isinstance(rhs_length, tuple):\n",
    "        lhs_precision, lhs_scale = lhs_length\n",
    "        rhs_precision, rhs_scale = rhs_length\n",
    "        result = lhs_precision <= rhs_precision and lhs_scale == rhs_scale\n",
    "        print(f\"  - Decimal check: LHS (Precision: {lhs_precision}, Scale: {lhs_scale}) | \"\n",
    "              f\"RHS (Precision: {rhs_precision}, Scale: {rhs_scale}) | Result = {result}\")\n",
    "        return result\n",
    "    \n",
    "    # Case 2: One is decimal, and the other is not (Type Mismatch)\n",
    "    if isinstance(lhs_length, tuple) or isinstance(rhs_length, tuple):\n",
    "        print(\"  - Type mismatch: One is decimal, the other is not. Returning False.\")\n",
    "        return False  # Integer/String cannot be compared with Decimal\n",
    "\n",
    "    # Case 3: General Numeric & String Length Comparison (Integer/String Lengths)\n",
    "    result = lhs_length <= rhs_length\n",
    "    print(f\"  - Integer/String length comparison: LHS = {lhs_length}, RHS = {rhs_length} | Result = {result}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_similarity_score(lhs_length, rhs_length):\n",
    "    print(f\"Original LHS Length: {lhs_length}, Original RHS Length: {rhs_length}\")\n",
    "    \n",
    "    lhs_length = normalize_length(lhs_length)\n",
    "    rhs_length = normalize_length(rhs_length)\n",
    "    \n",
    "    print(f\"Normalized LHS Length: {lhs_length}, Normalized RHS Length: {rhs_length}\")\n",
    "    \n",
    "    if lhs_length is None or rhs_length is None:\n",
    "        print(\"Invalid length encountered, returning similarity score of 0.\")\n",
    "        return 0  # Return 0 if any length is invalid\n",
    "\n",
    "    # Case 1: Both are decimal (precision, scale)\n",
    "    if isinstance(lhs_length, tuple) and isinstance(rhs_length, tuple):\n",
    "        lhs_precision, lhs_scale = lhs_length\n",
    "        rhs_precision, rhs_scale = rhs_length\n",
    "\n",
    "        print(f\"Decimal Case - LHS Precision: {lhs_precision}, LHS Scale: {lhs_scale}, \"\n",
    "              f\"RHS Precision: {rhs_precision}, RHS Scale: {rhs_scale}\")\n",
    "\n",
    "        if lhs_precision > rhs_precision:\n",
    "            print(\"LHS precision is greater than RHS precision, returning similarity score of 0.\")\n",
    "            return 0  # LHS precision is greater, no compatibility\n",
    "\n",
    "        # Calculate precision similarity\n",
    "        precision_diff = abs(lhs_precision - rhs_precision)\n",
    "        max_precision = max(lhs_precision, rhs_precision)\n",
    "        precision_score = 1 - (precision_diff / max_precision) if max_precision else 1\n",
    "        print(f\"Precision Difference: {precision_diff}, Precision Score: {precision_score}\")\n",
    "\n",
    "        # Calculate scale similarity\n",
    "        scale_diff = abs(lhs_scale - rhs_scale)\n",
    "        max_scale = max(lhs_scale, rhs_scale)\n",
    "        scale_score = 1 - (scale_diff / max_scale) if max_scale else 1\n",
    "        print(f\"Scale Difference: {scale_diff}, Scale Score: {scale_score}\")\n",
    "\n",
    "        # Average precision and scale similarity\n",
    "        final_score = (precision_score + scale_score) / 2\n",
    "        print(f\"Final Decimal Similarity Score: {final_score}\")\n",
    "        return final_score\n",
    "\n",
    "    # Case 2: One is decimal, the other is not (Type Mismatch)\n",
    "    if isinstance(lhs_length, tuple) or isinstance(rhs_length, tuple):\n",
    "        print(\"Type Mismatch: One is decimal and the other is not, returning similarity score of 0.\")\n",
    "        return 0  # Decimal vs Integer/String → No similarity\n",
    "\n",
    "    # Case 3: General Numeric & String Length Comparison\n",
    "    length_difference = abs(lhs_length - rhs_length)\n",
    "    max_length = max(lhs_length, rhs_length)\n",
    "    similarity_score = 1 - (length_difference / max_length) if max_length else 1\n",
    "\n",
    "    print(f\"Length Difference: {length_difference}, Max Length: {max_length}, Final Similarity Score: {similarity_score}\")\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Compatible data using Length and Data Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = load_mappings(mappings_dir)\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "compatibilities = load_compatibilities(compatibilities_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def filter_compatible_indices(df1, df2, type_mapping, compatibilities):\n",
    "    print(\"Starting compatibility filtering...\")\n",
    "\n",
    "    # Normalize data types and extract matching databases\n",
    "    df1['Normalized_Type'], df1['Matching_Databases'] = zip(*df1['Data_Type'].apply(lambda x: normalize_data_type(x, type_mapping)))\n",
    "    df2['Normalized_Type'], df2['Matching_Databases'] = zip(*df2['Data_Type'].apply(lambda x: normalize_data_type(x, type_mapping)))\n",
    "\n",
    "    print(\"Data types normalized for both datasets.\")\n",
    "\n",
    "    # Initialize dictionaries to store compatible indices and length scores\n",
    "    compatible_index = defaultdict(list)\n",
    "    length_score = defaultdict(list)\n",
    "\n",
    "    # Iterate over each field in df1\n",
    "    for idx1, row1 in df1.iterrows():\n",
    "        lhs_type = row1['Normalized_Type']\n",
    "        lhs_length = row1['Length']\n",
    "        print(f\"\\nChecking LHS Index: {idx1}, Type: {lhs_type}, Length: {lhs_length}\")\n",
    "\n",
    "        # Compare with each field in df2\n",
    "        for idx2, row2 in df2.iterrows():\n",
    "            rhs_type = row2['Normalized_Type']\n",
    "            rhs_length = row2['Length']\n",
    "            print(f\"  Comparing with RHS Index: {idx2}, Type: {rhs_type}, Length: {rhs_length}\")\n",
    "\n",
    "            # Check type and length compatibility\n",
    "            if are_compatible(lhs_type, rhs_type, compatibilities):\n",
    "                print(\"  - Data types are compatible.\")\n",
    "\n",
    "                if is_length_compatible(lhs_length, rhs_length, check_length=True):\n",
    "                    print(\"  - Lengths are compatible.\")\n",
    "\n",
    "                    score = length_similarity_score(lhs_length, rhs_length)\n",
    "                    print(f\"  - Length Similarity Score: {score}\")\n",
    "\n",
    "                    compatible_index[idx1].append(idx2)\n",
    "                    length_score[idx1].append(score)\n",
    "                else:\n",
    "                    print(\"  - Lengths are NOT compatible.\")\n",
    "            else:\n",
    "                print(\"  - Data types are NOT compatible.\")\n",
    "\n",
    "    print(\"\\nCompatibility filtering completed.\")\n",
    "    return compatible_index, length_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(column, batch_size, model):\n",
    "    embeddings = []\n",
    "    column = column.tolist()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(column), batch_size), desc=\"Encoding Batches\"):\n",
    "            batch = column[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(compatible_indices, df1, df2, column1, column2, model, calculated_similarities):\n",
    "    embeddings1 = batch_encode(df1[column1], batch_size=128, model=model)\n",
    "    embeddings2 = batch_encode(df2[column2], batch_size=128, model=model)\n",
    "\n",
    "    similarity_matrix = np.zeros((len(df1), len(df2)))\n",
    "\n",
    "    for idx1, compatible_idxs in compatible_indices.items():\n",
    "        for idx2 in compatible_idxs:\n",
    "            if (idx1, idx2) not in calculated_similarities:\n",
    "                similarity_score = util.cos_sim(embeddings1[idx1], embeddings2[idx2]).item()\n",
    "                similarity_matrix[idx1, idx2] = (similarity_score + 1) / 2  \n",
    "                calculated_similarities.add((idx1, idx2))\n",
    "\n",
    "    return csr_matrix(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native_type(value):\n",
    "    if isinstance(value, (np.integer, np.int64)):\n",
    "        return int(value)\n",
    "    elif isinstance(value, (np.floating, np.float64)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        return value.tolist()\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load mappings and compatibilities\n",
    "print(\"Loading type mappings...\")\n",
    "type_mapping = load_mappings(mappings_dir)\n",
    "print(f\"Type mappings loaded: {type_mapping}\")\n",
    "\n",
    "# Define the path to the compatibilities file\n",
    "compatibilities_file = os.path.join(mappings_dir, 'compatibilities.json')\n",
    "print(f\"Compatibilities file path: {compatibilities_file}\")\n",
    "\n",
    "# Load the compatibilities from the file\n",
    "print(\"Loading compatibilities...\")\n",
    "compatibilities = load_compatibilities(compatibilities_file)\n",
    "print(f\"Compatibilities loaded: {compatibilities}\")\n",
    "\n",
    "# Filter compatible data\n",
    "print(\"Filtering compatible indices based on type and length...\")\n",
    "compatible_indices, length_score = filter_compatible_indices(df1, df2, type_mapping, compatibilities)\n",
    "print(f\"Compatible indices: {compatible_indices}\")\n",
    "print(f\"Length score: {length_score}\")\n",
    "\n",
    "# Convert columns to string type\n",
    "column1 = 'Description'\n",
    "column2 = 'Description'\n",
    "print(f\"Converting column '{column1}' in df1 to string...\")\n",
    "df1[column1] = df1[column1].astype(str)\n",
    "print(f\"df1[{column1}] converted to string.\")\n",
    "\n",
    "print(f\"Converting column '{column2}' in df2 to string...\")\n",
    "df2[column2] = df2[column2].astype(str)\n",
    "print(f\"df2[{column2}] converted to string.\")\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Calculate similarity matrix\n",
    "calculated_similarities = set()\n",
    "\n",
    "print(\"Computing similarity matrix...\")\n",
    "sparse_similarity_matrix = calculate_similarities(compatible_indices, df1, df2, column1, column2, model, calculated_similarities)\n",
    "print(\"Similarity matrix computation completed.\")\n",
    "\n",
    "# Print summary of similarity matrix\n",
    "print(f\"Sparse similarity matrix: {sparse_similarity_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top similar Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, sparse_similarity_matrix, filtered_df1, filtered_df2, column1, column2, similarity_threshold, compatible_indices, length_scores):\n",
    "    print(f\"\\nRetrieving top {top_n} similar sentences for index: {selected_index}\")\n",
    "    \n",
    "    # Convert the sparse similarity matrix row for the selected index to a dense array and flatten it\n",
    "    similarities = sparse_similarity_matrix[selected_index].toarray().flatten()\n",
    "    print(f\"Similarity scores for index {selected_index}: {similarities}\")\n",
    "\n",
    "    # Initialize a list to store matches\n",
    "    matches = []\n",
    "\n",
    "    # Check if the selected index is in the compatible indices dictionary\n",
    "    if selected_index in compatible_indices:\n",
    "        print(f\"Index {selected_index} is found in compatible indices. Processing compatible indices...\")\n",
    "\n",
    "        # Iterate over the compatible indices for the selected index\n",
    "        for idx, length_score in zip(compatible_indices[selected_index], length_scores[selected_index]):   \n",
    "            similarity_score = similarities[idx]\n",
    "\n",
    "            # Skip if the similarity score is below the threshold\n",
    "            if similarity_score < similarity_threshold:\n",
    "                print(f\"Skipping index {idx} due to low similarity score ({similarity_score} < {similarity_threshold})\")\n",
    "                continue\n",
    "\n",
    "            # Retrieve relevant information from filtered_df2 for the current index\n",
    "            rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_desc = filtered_df2.loc[idx, column2]\n",
    "            rhs_type = filtered_df2.loc[idx, 'Data_Type']\n",
    "            rhs_length = filtered_df2.loc[idx, 'Length']\n",
    "            matching_databases = filtered_df2.loc[idx, 'Matching_Databases']\n",
    "\n",
    "            # Append the match information to the matches list\n",
    "            match_data = {\n",
    "                \"rank\": None,\n",
    "                \"similarity_score\": float(similarity_score),\n",
    "                \"rhs_index\": int(filtered_df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"matching_databases\": matching_databases if matching_databases else [\"Unknown\"],\n",
    "                \"rhs_length\": convert_to_native_type(rhs_length),\n",
    "                \"length_score\": float(length_score),\n",
    "                \"compatibility\": \"Compatible\"\n",
    "            }\n",
    "            print(f\"Appending match: {match_data}\")\n",
    "            matches.append(match_data)\n",
    "\n",
    "    # Sort the matches by similarity score in descending order and keep only the top N matches\n",
    "    matches = sorted(matches, key=lambda x: (x[\"similarity_score\"], x[\"length_score\"]), reverse=True)[:top_n]\n",
    "    \n",
    "    # Assign ranks to the matches\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "\n",
    "    # If no matches are found, add a message indicating no compatible matches\n",
    "    if not matches:\n",
    "        print(f\"No compatible matches found for index {selected_index}.\")\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Retrieve relevant information from filtered_df1 for the selected index\n",
    "    lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = filtered_df1.loc[selected_index, column1]\n",
    "    lhs_type = filtered_df1.loc[selected_index, 'Data_Type']\n",
    "    lhs_length = filtered_df1.loc[selected_index, 'Length']\n",
    "\n",
    "    print(f\"Selected LHS field details: Field Name: {lhs_field}, Description: {lhs_desc}, Data Type: {lhs_type}, Length: {lhs_length}\")\n",
    "\n",
    "    # Create the result dictionary containing the LHS field information and matches\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"lhs_field_length\": convert_to_native_type(lhs_length),\n",
    "        \"matches\": matches\n",
    "    }\n",
    "\n",
    "    print(f\"Final result for index {selected_index}: {result}\")\n",
    "\n",
    "    # Return the result wrapped in a dictionary with a \"results\" key\n",
    "    return {\"results\": [result]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Input with Validations\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "#similarity_threshold = float(input(\"Enter the similarity score threshold (0.0 - 1.0): \"))\n",
    "similarity_threshold = 0.0\n",
    "# Retrieve top similar sentences\n",
    "\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, sparse_similarity_matrix, df1, df2, column1, column2, similarity_threshold, compatible_indices, length_score)\n",
    "print(json.dumps(output_json, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_DataType_Length_Index_1.json\", \"w\") as json_file:     \n",
    "    json.dump(output_json, json_file, indent=4) \n",
    "    print(\"JSON file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert series to numeric values\n",
    "def convert_numeric_values(series):\n",
    "    try:\n",
    "        # Attempt to convert the series to numeric values, coercing errors to NaN\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "    except Exception as e:\n",
    "        # Print an error message if conversion fails\n",
    "        print(f\"Conversion error: {e}\")\n",
    "        return series\n",
    "\n",
    "# Function to compute statistical similarity between two columns\n",
    "def compute_statistical_similarity(lhs_col, rhs_col, dtype_category):\n",
    "    \"\"\"\n",
    "    Determines if two data distributions are statistically similar based on their type.\n",
    "    \"\"\"\n",
    "    # Initialize similarity score to 0\n",
    "    similarity_score = 0\n",
    "    \n",
    "    # Drop NaN values from both columns\n",
    "    lhs_values = lhs_col.dropna()\n",
    "    rhs_values = rhs_col.dropna()\n",
    "    \n",
    "    # Check if the data type category is numeric\n",
    "    if dtype_category in [\"INT\", \"INTEGER\", \"BIGINT\", \"SMALLINT\", \"FLOAT\", \"DOUBLE\", \"REAL\", \"NUMERIC\", \"DECIMAL\"]:\n",
    "        # Convert columns to numeric values\n",
    "        lhs_values = convert_numeric_values(lhs_col.dropna())\n",
    "        rhs_values = convert_numeric_values(rhs_col.dropna())\n",
    "        \n",
    "        # Compute Wasserstein distance and Kolmogorov-Smirnov statistic if both columns have values\n",
    "        if len(lhs_values) > 0 and len(rhs_values) > 0:\n",
    "            wasserstein_dist = stats.wasserstein_distance(lhs_values, rhs_values)\n",
    "            ks_stat, ks_p_value = stats.ks_2samp(lhs_values, rhs_values)\n",
    "            # Calculate similarity score based on Wasserstein distance and KS statistic\n",
    "            similarity_score = 1 / (1 + wasserstein_dist) if wasserstein_dist > 0 else 1\n",
    "            similarity_score = min(similarity_score, 1 - ks_stat)  # Normalize\n",
    "        else:\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    \n",
    "    # Check if the data type category is categorical\n",
    "    elif dtype_category in [\"CHAR\", \"VARCHAR\", \"TEXT\", \"STRING\", \"JSON\"]:\n",
    "        # Convert columns to unique string values\n",
    "        lhs_values = lhs_col.dropna().astype(str).unique()\n",
    "        rhs_values = rhs_col.dropna().astype(str).unique()\n",
    "        \n",
    "        # Compute Jaccard similarity if both columns have values\n",
    "        if len(lhs_values) > 0 and len(rhs_values) > 0:\n",
    "            vectorizer = CountVectorizer(preprocessor=lambda x: x, binary=True)\n",
    "            matrix = vectorizer.fit_transform([' '.join(lhs_values), ' '.join(rhs_values)])\n",
    "            similarity_score = 1 - jaccard(matrix.toarray()[0], matrix.toarray()[1])\n",
    "        else:\n",
    "            print(\"something happen\")\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    elif dtype_category in [\"DATE\", \"TIMESTAMP\", \"TIME\", \"DATETIME\"]:\n",
    "\n",
    "        # Convert columns to datetime values\n",
    "        lhs_values = pd.to_datetime(lhs_col, errors='coerce')\n",
    "        rhs_values = pd.to_datetime(rhs_col, errors='coerce')\n",
    "        \n",
    "        # Drop NaN values from both columns\n",
    "        lhs_values = lhs_values.dropna()\n",
    "        rhs_values = rhs_values.dropna()\n",
    "        \n",
    "        # Compute similarity score based on min and max date values\n",
    "        if not lhs_values.empty and not rhs_values.empty:\n",
    "            min_date_col1 = lhs_values.min()\n",
    "            max_date_col1 = lhs_values.max()\n",
    "            min_date_col2 = rhs_values.min()\n",
    "            max_date_col2 = rhs_values.max()\n",
    "            \n",
    "            # Calculate similarity score based on date range overlap\n",
    "            similarity_score = 1 - (abs((min_date_col1 - min_date_col2).days) + abs((max_date_col1 - max_date_col2).days)) / 365\n",
    "        else:\n",
    "            similarity_score = 0  # No valid comparison\n",
    "    elif dtype_category in [\"BOOLEAN\"]:\n",
    "        # Convert columns to boolean values\n",
    "        lhs_values = lhs_col.dropna().astype(bool)\n",
    "        rhs_values = rhs_col.dropna().astype(bool)\n",
    "        \n",
    "        # Compute similarity score based on Jaccard similarity of boolean values\n",
    "        similarity_score = jaccard_score(lhs_values, rhs_values)\n",
    "    else:\n",
    "        print(\"NOT WORK FOR THIS DATA TYPE\")\n",
    "        similarity_score = 0  # Incompatible types\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "# Function to truncate percentage values to two decimal places\n",
    "def truncate_percentage(value):\n",
    "    return f\"{value:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top similar sentences based on similarity and statistical compatibility\n",
    "def retrieve_top_similar_sentences_json(selected_index, top_n, sparse_similarity_matrix, filtered_df1, filtered_df2, column1, column2, similarity_threshold, statistical_threshold, compatible_indices, length_scores):\n",
    "    # Convert the sparse similarity matrix row for the selected index to a dense array and flatten it\n",
    "    similarities = sparse_similarity_matrix[selected_index].toarray().flatten()\n",
    "    # Initialize a list to store matches\n",
    "    matches = []\n",
    "\n",
    "    # Check if the selected index is in the compatible indices dictionary\n",
    "    if selected_index in compatible_indices:\n",
    "        # Iterate over the compatible indices for the selected index\n",
    "        for idx, length_score in zip(compatible_indices[selected_index], length_scores[selected_index]):  \n",
    "            # Get the similarity score for the current index\n",
    "            similarity_score = similarities[idx]\n",
    "            # Skip if the similarity score is below the threshold\n",
    "            if similarity_score < similarity_threshold:\n",
    "                continue\n",
    "\n",
    "            # Retrieve relevant information from filtered_df2 for the current index\n",
    "            rhs_field = filtered_df2.loc[idx, 'Attribute']\n",
    "            rhs_desc = filtered_df2.loc[idx, column2]\n",
    "            rhs_type = filtered_df2.loc[idx, 'Data_Type']\n",
    "            normalized_rhs_type = filtered_df2.loc[idx, 'Normalized_Type']\n",
    "            rhs_length = filtered_df2.loc[idx, 'Length']\n",
    "            matching_databases = filtered_df2.loc[idx, 'Matching_Databases']\n",
    "            \n",
    "            # Retrieve the field name from filtered_df1 for the selected index\n",
    "            lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "\n",
    "            # Check if the lhs_field and rhs_field exist in df3 and df4 respectively\n",
    "            if lhs_field in df3.columns and rhs_field in df4.columns:\n",
    "                # Retrieve the columns from df3 and df4\n",
    "                lhs_col = df3[lhs_field]\n",
    "                rhs_col = df4[rhs_field]\n",
    "                # Compute the statistical similarity between the columns\n",
    "                stat_similarity = compute_statistical_similarity(lhs_col, rhs_col, normalized_rhs_type)\n",
    "            else:\n",
    "                stat_similarity = 0  # No valid comparison\n",
    "                \n",
    "            # Convert the statistical similarity to a percentage\n",
    "            stat_similarity_percentage = truncate_percentage(stat_similarity * 100)\n",
    "            \n",
    "            # Skip if the statistical similarity is below the threshold\n",
    "            if stat_similarity < statistical_threshold:\n",
    "                continue\n",
    "            \n",
    "                \n",
    "            # Append the match information to the matches list\n",
    "            matches.append({\n",
    "                \"rank\": None,\n",
    "                \"similarity_score\": float(similarity_score),\n",
    "                \"rhs_index\": int(filtered_df2.index[idx]),\n",
    "                \"rhs_field_name\": rhs_field,\n",
    "                \"rhs_field_desc\": rhs_desc,\n",
    "                \"rhs_data_type\": rhs_type,\n",
    "                \"matching_databases\": matching_databases if matching_databases else [\"Unknown\"],\n",
    "                \"rhs_length\": convert_to_native_type(rhs_length),\n",
    "                \"length_score\": float(length_score),\n",
    "                \"statistical_similarity\": stat_similarity_percentage  \n",
    "            })\n",
    "\n",
    "    # Sort the matches by similarity score in descending order and keep only the top N matches\n",
    "    #matches = sorted(matches, key=lambda x: x[\"similarity_score\"], reverse=True)[:top_n]\n",
    "    matches = sorted(matches, key=lambda x: (x[\"similarity_score\"],x[\"length_score\"], x[\"statistical_similarity\"]), reverse=True)[:top_n]\n",
    "\n",
    "    # Assign ranks to the matches\n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        match[\"rank\"] = rank\n",
    "\n",
    "    # If no matches are found, add a message indicating no compatible matches\n",
    "    if not matches:\n",
    "        matches.append({\"message\": \"No compatible matches found.\"})\n",
    "\n",
    "    # Retrieve relevant information from filtered_df1 for the selected index\n",
    "    lhs_field = filtered_df1.loc[selected_index, 'Field Name']\n",
    "    lhs_desc = filtered_df1.loc[selected_index, column1]\n",
    "    lhs_type = filtered_df1.loc[selected_index, 'Data_Type']\n",
    "    lhs_length = filtered_df1.loc[selected_index, 'Length']\n",
    "\n",
    "    # Create the result dictionary containing the LHS field information and matches\n",
    "    result = {\n",
    "        \"lhs_field_index\": selected_index,\n",
    "        \"lhs_field_name\": lhs_field,\n",
    "        \"lhs_field_description\": lhs_desc,\n",
    "        \"lhs_field_data_type\": lhs_type,\n",
    "        \"lhs_field_length\": convert_to_native_type(lhs_length),\n",
    "        \"matches\": matches\n",
    "    }\n",
    "\n",
    "    # Return the result wrapped in a dictionary with a \"results\" key\n",
    "    return {\"results\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Input with Validations\n",
    "enter_index = int(input(\"Enter the index from LHS to process: \"))\n",
    "top_n = int(input(\"Enter the number of top similar sentences to retrieve (N): \") or 3)\n",
    "#similarity_threshold = float(input(\"Enter the similarity score threshold (0.0 - 1.0): \"))\n",
    "#statistical_threshold = float(input(\"Enter the statistical similarity score threshold (0.0 - 1.0): \"))\n",
    "\n",
    "similarity_threshold = 0.5\n",
    "statistical_threshold = 0.5\n",
    "\n",
    "# Retrieve top similar sentences\n",
    "output_json = retrieve_top_similar_sentences_json(\n",
    "    enter_index, top_n, sparse_similarity_matrix, df1, df2, column1, column2, similarity_threshold,statistical_threshold, compatible_indices, length_score)\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_statistics_compatibility_Index_1.json\", \"w\") as json_file:     \n",
    "    json.dump(output_json, json_file, indent=4) \n",
    "    print(\"JSON file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import IPython.display as display\n",
    "\n",
    "\n",
    "# Load data into DataFrames\n",
    "lhs_df = df3\n",
    "rhs_df = df4\n",
    "\n",
    "# Define columns of interest\n",
    "lhs_column = output_json[\"results\"][0][\"lhs_field_name\"]\n",
    "if \"message\" not in output_json[\"results\"][0][\"matches\"][0]:\n",
    "    rhs_columns = [match[\"rhs_field_name\"] for match in output_json[\"results\"][0][\"matches\"]]\n",
    "else:\n",
    "    rhs_columns = []\n",
    "\n",
    "# Extract relevant columns\n",
    "lhs_data = lhs_df[[lhs_column]]\n",
    "rhs_data = rhs_df[rhs_columns]\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = pd.DataFrame(columns=[\"Column\", \"Null Values\", \"Distinct Values\", \"Non-Distinct Values\", \"Most Common Value\", \"Mean Length\", \"Min Length\", \"Max Length\"])\n",
    "\n",
    "for col in [lhs_column] + rhs_columns:\n",
    "    null_values = lhs_df[col].isnull().sum() if col == lhs_column else rhs_df[col].isnull().sum()\n",
    "    distinct_values = lhs_df[col].nunique() if col == lhs_column else rhs_df[col].nunique()\n",
    "    total_values = len(lhs_df[col]) if col == lhs_column else len(rhs_df[col])\n",
    "    non_distinct_values = total_values - distinct_values\n",
    "    most_common_value = lhs_df[col].mode()[0] if col == lhs_column else rhs_df[col].mode()[0]\n",
    "    \n",
    "    mean_length = lhs_df[col].astype(str).str.len().mean() if col == lhs_column else rhs_df[col].astype(str).str.len().mean()\n",
    "    min_length = lhs_df[col].astype(str).str.len().min() if col == lhs_column else rhs_df[col].astype(str).str.len().min()\n",
    "    max_length = lhs_df[col].astype(str).str.len().max() if col == lhs_column else rhs_df[col].astype(str).str.len().max()\n",
    "    \n",
    "    summary_stats.loc[len(summary_stats)] = [col, null_values, distinct_values, non_distinct_values, most_common_value, mean_length, min_length, max_length]\n",
    "\n",
    "# Display summary statistics\n",
    "display.display(summary_stats)\n",
    "\n",
    "# Generate detailed view with inferred patterns and data types\n",
    "detailed_view = pd.DataFrame(columns=[\"Column\", \"Data Type\", \"Top Patterns\", \"Inferred Data Domain\"])\n",
    "\n",
    "for col in [lhs_column] + rhs_columns:\n",
    "    data_type = lhs_df[col].dtype if col == lhs_column else rhs_df[col].dtype\n",
    "    inferred_patterns = lhs_df[col].astype(str).str.extract(r'([A-Za-z]+)')[0].dropna().unique() if col == lhs_column else rhs_df[col].astype(str).str.extract(r'([A-Za-z]+)')[0].dropna().unique()\n",
    "    inferred_data_domain = \"Categorical\" if data_type == 'object' else \"Numerical\"\n",
    "    \n",
    "    detailed_view.loc[len(detailed_view)] = [col, data_type, inferred_patterns, inferred_data_domain]\n",
    "\n",
    "# Display detailed view\n",
    "display.display(detailed_view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
